{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4a56b0",
   "metadata": {},
   "source": [
    "All moved to [notebook](https://github.com/nnkennard/lab-notebook/blob/master/08/2021-08-14.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a08a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97a4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotator(annotation):\n",
    "    return annotation[\"metadata\"][\"anno\"]\n",
    "\n",
    "def get_rebuttal_alignments(annotation):\n",
    "    return [set(x[\"labels\"][\"alignments\"]) for x in annotation[\"rebuttallabels\"]]\n",
    "\n",
    "dataset_path = \"/Users/nnayak/Downloads/05-16-provisional-emnlp-release-unsplit/\"\n",
    "\n",
    "results = collections.defaultdict(list)\n",
    "adjudicated_results = collections.defaultdict(list)\n",
    "all_results = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "\n",
    "for filename in glob.glob(dataset_path + \"/*\"):\n",
    "    with open(filename, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "    review = obj[\"metadata\"][\"review\"]\n",
    "    if get_annotator(obj) == 'anno0':\n",
    "        adjudicated_results[review].append(obj)\n",
    "    else:\n",
    "        results[review].append(obj)\n",
    "    all_results[review].append(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa330be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "partial_matches = []\n",
    "js_distances = []\n",
    "\n",
    "class OverlapType(object):\n",
    "    BOTH_NONE = \"Agree none\"\n",
    "    ONE_NONE = \"Disagree none\"\n",
    "    PARTIAL = \"Partial match\"\n",
    "    NO_OVERLAP = \"No overlap\"\n",
    "    EXACT = \"Exact match\"\n",
    "    \n",
    "    ALL = [\n",
    "        EXACT,\n",
    "    BOTH_NONE,\n",
    "    PARTIAL,\n",
    "    ONE_NONE,\n",
    "    NO_OVERLAP,\n",
    "    ]\n",
    "\n",
    "def js_distance(set1, set2):\n",
    "    return len(set1.intersection(set2))/len(set1.union(set2))\n",
    "\n",
    "overlap_counter = collections.Counter()\n",
    "exact_counter = collections.Counter()\n",
    "\n",
    "for review, annotations in results.items():\n",
    "    if len(annotations) == 1:\n",
    "        continue\n",
    "    else:\n",
    "        annotators = [get_annotator(x) for x in annotations]\n",
    "        for annotation_1, annotation_2 in itertools.combinations(annotations, 2):\n",
    "            for ali_1, ali_2 in zip(get_rebuttal_alignments(annotation_1), get_rebuttal_alignments(annotation_2)):\n",
    "                if not (ali_1.union(ali_2)):\n",
    "                    overlap_counter[OverlapType.BOTH_NONE] += 1\n",
    "                elif not ali_1 or not ali_2:\n",
    "                    overlap_counter[OverlapType.ONE_NONE] += 1\n",
    "                else:\n",
    "                    if ali_1 == ali_2:\n",
    "                        overlap_counter[OverlapType.EXACT] += 1\n",
    "                        exact_counter[len(ali_1)] += 1\n",
    "                    elif not (ali_1.intersection(ali_2)):\n",
    "                        overlap_counter[OverlapType.NO_OVERLAP] += 1\n",
    "                    else:\n",
    "                        overlap_counter[OverlapType.PARTIAL] += 1\n",
    "                        js_distances.append(js_distance(ali_1, ali_2))\n",
    "\n",
    "barplot_dicts = []\n",
    "for k, v in overlap_counter.items():\n",
    "    barplot_dicts.append({\n",
    "        \"Overlap type\": k,\n",
    "        \"Count\": v\n",
    "    })\n",
    "overlap_type_df = pd.DataFrame.from_dict(barplot_dicts)\n",
    "\n",
    "print(overlap_type_df)\n",
    "\n",
    "#overlap_type_df\n",
    "sns.barplot(data=overlap_type_df, x = \"Overlap type\", y=\"Count\")\n",
    "\n",
    "\n",
    "sns.histplot(js_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextType(object):\n",
    "    NO_CONTEXT = \"No context\"\n",
    "    GLOBAL_CONTEXT = \"Global context\"\n",
    "    SINGLE_SENTENCE = \"Single sentence\"\n",
    "    CONTIGUOUS_SENTENCES = \"Contiguous sentences\"\n",
    "    NONCONTIGUOUS_SENTENCES = \"Non-contiguous sentences\"\n",
    "    MYSTERIOUS = \"Mysterious\"\n",
    "    \n",
    "def is_contiguous(alignments):\n",
    "    relevant_range = list(range(min(alignments), max(alignments) + 1))\n",
    "    return relevant_range == list(sorted(alignments))\n",
    "\n",
    "for review, annotations in adjudicated_results.items():\n",
    "    for annotation in annotations:\n",
    "        if not get_annotator(annotation) == \"anno0\":\n",
    "            continue\n",
    "        for label in annotation[\"rebuttallabels\"]:\n",
    "            alignment = label[\"labels\"][\"alignments\"]\n",
    "            if not alignment:\n",
    "                context_counter[ContextType.MYSTERIOUS] += 1\n",
    "            elif len(alignment) == 1:\n",
    "                context_counter[ContextType.SINGLE_SENTENCE] += 1\n",
    "            else:\n",
    "                if is_contiguous(alignment):\n",
    "                    context_counter[ContextType.CONTIGUOUS_SENTENCES] += 1\n",
    "                else:\n",
    "                    context_counter[ContextType.NONCONTIGUOUS_SENTENCES] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count double annotations\n",
    "adjudicated_test_dir = \"/Users/nnayak/Downloads/0517_split_2/test/\"\n",
    "\n",
    "annotators = collections.Counter()\n",
    "for filename in glob.glob(adjudicated_test_dir+\"/*\"):\n",
    "    annotator = filename.split(\".\")[-2]\n",
    "    annotators[annotator] += 1\n",
    "\n",
    "#print(annotators, sum(annotators.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da252ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "# for example in sum(all_results.values(), []):\n",
    "#     review_coarse_labels = [label[\"labels\"][\"coarse\"] for label in example[\"reviewlabels\"]]\n",
    "#     for rebuttal_label in example[\"rebuttallabels\"]:\n",
    "#         coarse = rebuttal_label[\"labels\"][\"responsetype\"]\n",
    "#         for aligned_idx in rebuttal_label[\"labels\"][\"alignments\"]:\n",
    "#             j[coarse][review_coarse_labels[aligned_idx]] += 1\n",
    "            \n",
    "# print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a714c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=22, ncols=1, figsize=(10,50))\n",
    "\n",
    "# review_types = \"Request Evaluative Fact Structuring Other\".split()\n",
    "\n",
    "# for i, key in enumerate(sorted(j.keys())):\n",
    "\n",
    "#     vals = j[key]\n",
    "# #     print(vals)\n",
    "# #     print(i, key)\n",
    "#     axes[i].bar(review_types, [vals[i] for i in review_types])\n",
    "#     axes[i].set_ylabel(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in sum(all_results.values(), []):\n",
    "#     for i, rebuttal_label in enumerate(example[\"rebuttallabels\"]):\n",
    "#         if rebuttal_label[\"labels\"][\"responsetype\"] == \"followup\":\n",
    "#             print(example[\"rebuttal\"][i]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# combis = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "\n",
    "# agrees = collections.defaultdict(lambda:collections.Counter())\n",
    "# disagrees = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "# total_sentence_count = collections.Counter()\n",
    "\n",
    "# FIELDS = \"annotators label_type label_1 label_2 review_id sent_index sentence\".split()\n",
    "# DisagreementSentence = collections.namedtuple(\"DisagreementSentence\", FIELDS)\n",
    "\n",
    "# with open(\"disagree_sentences.tsv\", 'w') as f:\n",
    "#     writer = csv.DictWriter(f, fieldnames=FIELDS, delimiter=\"\\t\")\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for rev_id, annotations in all_results.items():\n",
    "#         if len(annotations) == 1:\n",
    "#             anno = annotations[0][\"metadata\"][\"anno\"]\n",
    "#             total_sentence_count[anno] += len(annotations[0][\"review\"])\n",
    "#         else:\n",
    "#             for ann_1, ann_2 in itertools.combinations(annotations, 2):\n",
    "#                 annos = sorted([ann_1[\"metadata\"][\"anno\"], ann_2[\"metadata\"][\"anno\"]], key=lambda x:int(x[4:]))\n",
    "#                 rev_1 = [i[\"labels\"] for i in ann_1[\"reviewlabels\"]]\n",
    "#                 rev_2 = [i[\"labels\"] for i in ann_2[\"reviewlabels\"]]\n",
    "#                 sentences = [i[\"sentence\"] for i in ann_1[\"review\"]]\n",
    "#                 assert len(rev_1) == len(rev_2)\n",
    "#                 for i, (labels_1, labels_2, sent) in enumerate(zip(rev_1, rev_2, sentences)):\n",
    "#                     for label_type in \"coarse fine asp pol\".split():\n",
    "#                         labels = sorted(\n",
    "#                                 [labels_1[label_type], labels_2[label_type]])\n",
    "#                         combis[label_type][tuple(labels)] += 1\n",
    "#                         if len(set(labels)) > 1:\n",
    "#                             disagrees[annos[0]][annos[1]]  += 1\n",
    "#                             writer.writerow(\n",
    "#                                 DisagreementSentence(\n",
    "#                                     \"_\".join(annos), label_type, labels[0], labels[1], rev_id, i, sent\n",
    "#                                 )._asdict()\n",
    "#                             )\n",
    "#                         else:\n",
    "#                             assert len(set(labels)) == 1\n",
    "#                             agrees[annos[0]][annos[1]]  += 1\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# annotators = [\"anno{0}\".format(i) for i in range(17)]\n",
    "# annotators= \"anno0 anno2 anno3 anno10 anno13 anno14\".split()\n",
    "\n",
    "# disagrees_df = np.array([[disagrees[i][j]/(agrees[i][j] + disagrees[i][j] + 1) for j in annotators] for i in annotators])\n",
    "# sns.heatmap(data=disagrees_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f444f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrees_total = 0\n",
    "# for k, v in agrees.items():\n",
    "#     agrees_total += sum(v.values())\n",
    "# disagrees_total = 0\n",
    "# for k, v in disagrees.items():\n",
    "#     disagrees_total += sum(v.values())\n",
    "\n",
    "# print(agrees_total, disagrees_total)\n",
    "\n",
    "# for anno1, annos in agrees.items():\n",
    "#     for anno2, count in annos.items():\n",
    "#         print(anno1, anno2, count, disagrees[anno1][anno2])\n",
    "\n",
    "# for k, v in total_sentence_count.items():\n",
    "#     print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
