{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ea656",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_template = \"\"\"\n",
    "<HTML>\n",
    "<head> <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.2/css/bulma.min.css\">\n",
    "</head>\n",
    "<body>{0}</body>\n",
    "</HTML>\"\"\"\n",
    "\n",
    "def build_label_col(info, max_len, color, model):\n",
    "    cells = []\n",
    "    for i in range(max_len):\n",
    "        if i+1 in info[\"top_ranked_indices\"]:\n",
    "            cells.append(\"<td bgcolor={0}>{1}</td>\".format(color, model))\n",
    "        else:\n",
    "            cells.append(\"<td></td>\")\n",
    "\n",
    "    return cells\n",
    "\n",
    "def build_text_col(text, max_len, highlighted_cols, color):\n",
    "    cells = []\n",
    "    for i in range(max_len):\n",
    "        if i < len(text):\n",
    "            this_cell_text = text[i]\n",
    "        else:\n",
    "            this_cell_text = \"\"\n",
    "        if i in highlighted_cols:\n",
    "            cells.append(\"<td bgcolor={0}>{1}</td>\".format(color, this_cell_text))\n",
    "        else:\n",
    "            cells.append(\"<td>{0}</td>\".format(this_cell_text))\n",
    "\n",
    "    return cells\n",
    "\n",
    "def build_table(rank_info, bm25_info, review_sentences, rebuttal_sentences):\n",
    "    total_lines = max([len(review_sentences), len(rebuttal_sentences)])\n",
    "    columns = \"review_text true_label bm25_label rank_label rebuttal_sentence\".split()\n",
    "    header = \"<tr>\" + \" \".join([\"<td>{0}</td>\".format(col) for col in columns]) + \"</td>\"\n",
    "    table_lines = [header]\n",
    "    rank_col = build_label_col(rank_info, total_lines, '#9FE2BF', \"rank\")\n",
    "    bm25_col = build_label_col(bm25_info, total_lines, '#40E0D0', \"bm25\")\n",
    "    assert rank_info[\"actual_labels\"] == bm25_info[\"actual_labels\"]\n",
    "    assert rank_info[\"rebuttal_idx\"] == bm25_info[\"rebuttal_idx\"]\n",
    "    review_col = build_text_col(review_sentences, total_lines, rank_info[\"actual_labels\"], '#6495ED')\n",
    "    rebuttal_col = build_text_col(rebuttal_sentences, total_lines, [rank_info[\"rebuttal_idx\"]], '#CCCCFF')\n",
    "                           \n",
    "    rows = [\" \".join([\"<tr>\"] + [b,c,a,d] + [\"</tr>\"] ) for a,b,c,d in zip(review_col, rank_col, bm25_col, rebuttal_col)]\n",
    "    \n",
    "    \n",
    "    preceding = rank_info[\"review_id\"] + \" \" + str(rank_info[\"rebuttal_idx\"]) + \"<br/><br/>\"\n",
    "    \n",
    "    table_text = preceding + '<table border=\"1px grey\">' + \"\\n\".join(rows) + \"</table>\"\n",
    "    return table_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f110336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "import json\n",
    "\n",
    "relevant_info = collections.defaultdict(dict)\n",
    "\n",
    "with open(\"ir_errors/mrr_errors_all.csv\", 'r') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        key = (obj[\"review_id\"], obj[\"rebuttal_idx\"])\n",
    "        assert obj[\"model\"] not in relevant_info[key]\n",
    "        relevant_info[key][obj[\"model\"]] = obj\n",
    "        \n",
    "text_map = {}\n",
    "for dev_file in glob.glob(\n",
    "    \"/Users/nnayak/Downloads/0517_split_2/dev/*\"):\n",
    "    with open(dev_file, 'r') as f:\n",
    "        obj = json.load(f)\n",
    "        review_text = [x[\"sentence\"] for x in obj[\"review\"]]\n",
    "        rebuttal_text = [x[\"sentence\"] for x in obj[\"rebuttal\"]]\n",
    "        text_map[obj[\"metadata\"][\"review\"]] = (review_text, rebuttal_text)\n",
    "\n",
    "table_texts = collections.defaultdict(dict)\n",
    "for (review_id, rebuttal_index), objs in relevant_info.items():\n",
    "    review_text, rebuttal_text = text_map[review_id]\n",
    "    table_texts[review_id][rebuttal_index] = build_table(objs[\"rank\"], objs[\"bm25\"], review_text, rebuttal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b39a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for review_id, tables in table_texts.items():\n",
    "    ordered_tables = \"<br/> <br/>\".join([tables[i] for i in sorted(tables.keys())])\n",
    "    with open(\"ir_output/\"+review_id + \"_tables.html\", 'w') as f:\n",
    "        f.write(html_template.format(ordered_tables))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3489111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_none_mean(l):\n",
    "    ll = [i for i in l if i is not None]\n",
    "    return sum(ll)/len(ll)\n",
    "\n",
    "rank_results = []\n",
    "bm25_results = []\n",
    "with open(\"ir_errors/mrr_results_all.csv\", 'r') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        rank_results.append(obj[\"rank_mrr\"])\n",
    "        bm25_results.append(obj[\"bm25_mrr\"])\n",
    "        \n",
    "        \n",
    "print(\"rank\", filter_none_mean(rank_results))\n",
    "print(\"bm25\", filter_none_mean(bm25_results))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc426f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
