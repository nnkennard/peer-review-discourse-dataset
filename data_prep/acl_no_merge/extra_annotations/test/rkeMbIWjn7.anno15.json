{
  "metadata": {
    "forum_id": "H1eqjiCctX",
    "review_id": "rkeMbIWjn7",
    "rebuttal_id": "Skg5uUslRX",
    "title": "Understanding Composition of Word Embeddings via Tensor Decomposition",
    "reviewer": "AnonReviewer2",
    "rating": 7,
    "conference": "ICLR2019",
    "permalink": "https://openreview.net/forum?id=H1eqjiCctX&noteId=Skg5uUslRX",
    "annotator": "anno15"
  },
  "review_sentences": [
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 0,
      "text": "The paper deals with further development of RAND-WALK model of Arora et al. There are stable idioms, adjective-noun pairs and etc that are not covered by RAND-WALK, because sometimes words from seemingly different contexts can join to form a stable idiom.",
      "suffix": "\n\n",
      "coarse": "arg_evaluative",
      "fine": "none",
      "asp": "asp_motivation-impact",
      "pol": "pol_positive"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 1,
      "text": "So, the idea of paper is to introduce a tensor T and a stable idiom (a,b) is embedded into v_{ab}=v_a+v_b+T(v_a, v_b,.) and is emitted with some probability p_sym (proportional to exp(v_{ab} times context)).",
      "suffix": "",
      "coarse": "arg_structuring",
      "fine": "arg-structuring_summary",
      "asp": "none",
      "pol": "none"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 2,
      "text": "The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.",
      "suffix": "",
      "coarse": "arg_evaluative",
      "fine": "none",
      "asp": "asp_originality",
      "pol": "pol_neutral"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 3,
      "text": "Finally, there exists an expression, PMI3(u,v,w), that shows the correlation between 3 words, and that can be estimated from the data directly.",
      "suffix": "",
      "coarse": "arg_structuring",
      "fine": "arg-structuring_summary",
      "asp": "none",
      "pol": "none"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 4,
      "text": "It is proved that Tucker decomposition of that tensor gives us all words embeddings together with tensor T. Thus, from the latter we will obtain a tool for finding embeddings of idioms (i.e. v_a+v_b+T(v_a, v_b,.)).",
      "suffix": "\n\n",
      "coarse": "arg_evaluative",
      "fine": "none",
      "asp": "asp_replicability",
      "pol": "pol_positive"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 5,
      "text": "Theoretical analysis seems correct (I have not checked all the statements thoroughly, but I would expect formulations to be true).",
      "suffix": "",
      "coarse": "arg_evaluative",
      "fine": "none",
      "asp": "asp_soundness-correctness",
      "pol": "pol_positive"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 6,
      "text": "The only problem I see is that phrase similarity part is not convincing.",
      "suffix": "",
      "coarse": "arg_evaluative",
      "fine": "none",
      "asp": "asp_soundness-correctness",
      "pol": "pol_negative"
    },
    {
      "review_id": "rkeMbIWjn7",
      "sentence_index": 7,
      "text": "I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.",
      "suffix": "",
      "coarse": "arg_evaluative",
      "fine": "none",
      "asp": "asp_substance",
      "pol": "pol_negative"
    }
  ],
  "rebuttal_sentences": [
    {
      "review_id": "rkeMbIWjn7",
      "rebuttal_id": "Skg5uUslRX",
      "sentence_index": 0,
      "text": "We thank the reviewer for their time and response to our paper.",
      "suffix": "\n\n",
      "coarse": "nonarg",
      "fine": "rebuttal_social",
      "alignment": [
        "context_global",
        null
      ],
      "details": {}
    },
    {
      "review_id": "rkeMbIWjn7",
      "rebuttal_id": "Skg5uUslRX",
      "sentence_index": 1,
      "text": "Phrase similarity results: the tensor component T(v_a,v_b,.) does yield improvement over all other weighted additive methods in 5 out of 6 cases, as shown in Table 3.",
      "suffix": "",
      "coarse": "dispute",
      "fine": "rebuttal_refute-question",
      "alignment": [
        "context_sentences",
        [
          6,
          7
        ]
      ],
      "details": {}
    },
    {
      "review_id": "rkeMbIWjn7",
      "rebuttal_id": "Skg5uUslRX",
      "sentence_index": 2,
      "text": "We have also updated that table with additional results, which show that adding in the tensor component improves upon the strong baseline of the SIF embedding method.",
      "suffix": "",
      "coarse": "concur",
      "fine": "rebuttal_done",
      "alignment": [
        "context_sentences",
        [
          6,
          7
        ]
      ],
      "details": {
        "request_out_of_scope": true
      }
    },
    {
      "review_id": "rkeMbIWjn7",
      "rebuttal_id": "Skg5uUslRX",
      "sentence_index": 3,
      "text": "We also added Table 4, which repeats the phrase-similarity task for verb-object pairs, and shows that the tensor component leads to improvement in most cases.",
      "suffix": "",
      "coarse": "concur",
      "fine": "rebuttal_done",
      "alignment": [
        "context_sentences",
        [
          6,
          7
        ]
      ],
      "details": {
        "request_out_of_scope": true
      }
    }
  ]
}