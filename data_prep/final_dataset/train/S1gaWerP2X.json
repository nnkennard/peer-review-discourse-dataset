{"metadata": {"forum_id": "B1xsqj09Fm", "review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "reviewer": "AnonReviewer2", "rating": 7, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=B1xsqj09Fm&noteId=r1gI_-SQAm", "annotator": "anno3"}, "review_sentences": [{"review_id": "S1gaWerP2X", "sentence_index": 0, "text": "Summary:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 1, "text": "The authors present a empirical investigation of methods for scaling GANs to complex datasets, such as ImageNet, for class-conditioned image generation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 2, "text": "They first build and describe a strong baseline based on recently proposed techniques for GANs and push the performance on large datasets with several modifications presented sequentially, to obtain strong state-of-the-art IS/FID scores, as well as impressive visual results.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 3, "text": "The authors propose a simple truncation trick to control the fidelity/variance which is interesting on its own but cannot always scale with the architecture.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 4, "text": "The authors further propose a orthogonalization-based regularization to mitigate this problem.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 5, "text": "An investigation of training collapse at large scale is also performed; the authors investigate some regularization schemes based on gathered empirical evidence.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 6, "text": "As a result, they explore and discard Spectral Normalization of the generator as a way to prevent collapse and show that a severe tradeoff between stability and quality can be controlled when using zero-centered gradient penalties in the Discriminator.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 7, "text": "In the end, no solution that can ensure quality and stability is found, except having prohibitively large amounts of data (~300M images).", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 8, "text": "Models are evaluated on the ImageNet and on this internal, bigger dataset.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 9, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 10, "text": "- This investigation gives a significant amount of insights on GAN stability and performance at large scales, which should be useful for anyone working with GANs on complex datasets (and that have access to great computational resources).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 11, "text": "- Even though commonly used evaluations metrics for GANs are still not fully adequate, the authors obtain quantitative performance significantly beyond previous work, which seems indeed correlated with remarkable visual results.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 12, "text": "- The baseline and added modifications are well presented and clearly explained.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 13, "text": "The Appendices also have great value in that regard.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 14, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 15, "text": "- Discussions sometimes lack depth or are absent.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 16, "text": "For example, it is unclear to me why some larger models are not amenable to truncation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 17, "text": "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts? Were samples from those networks better without using truncation? Why would this be?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 19, "text": "Authors report how wider networks perform best, and how deeper networks degrade performance.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 20, "text": "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 21, "text": "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 22, "text": "- In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear. Is this something the reader should understand from Table 1?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 24, "text": "- I question the choice of sections chosen to be in the main paper/appendices.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 25, "text": "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 26, "text": "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 27, "text": "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 28, "text": "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 29, "text": "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 30, "text": "Suggestions/Comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 31, "text": "- Regarding the diversity/fidelity tradeoff using different truncation thresholds, I think constraining the norm of the sampled noise vectors to the exact threshold value (by projecting the samples on the 0-centered hyper-sphere of radius = threshold) could yield even more interesting or more informative Figures, as obtained scores or samples on the edge of that hyper-sphere might provide information on the \u2018guaranteed\u2019 (not proven) quality/fidelity of samples mapped from inside that hyper-sphere.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 32, "text": "- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 33, "text": "Similar curves could also be produced with the hyper-sphere projection proposed above to have a slightly clearer idea of the behavior on the limit of that hyper-sphere.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 34, "text": "- In Section 4.2, in the second paragraph, you refer to Appendix F and describe \u201csharp upward jump at collapse\u201d in D\u2019s loss.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 35, "text": "However, it seems the only Figure showing D\u2019s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 36, "text": "- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 37, "text": "This investigation of GAN scalability is successful results-wise even though the inability to stabilize training without sacrificing great performance on ImageNet is disappointing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gaWerP2X", "sentence_index": 38, "text": "The improvement over previous SOTA is definitely significant.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gaWerP2X", "sentence_index": 39, "text": "This work thus shows a modern GAN architecture for complex datasets that could be a strong basis for future work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 40, "text": "However, I think the paper could and should be improved with some more detailed analysis and discussions of exhibited behaviors in order to further guide and encourage future work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 41, "text": "It could also be clarified on some aspects, and potentially re-structured a bit to be better align with its probable impact directions.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1gaWerP2X", "sentence_index": 42, "text": "I would also be curious to see the proposed techniques applied on simpler datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_neutral"}, {"review_id": "S1gaWerP2X", "sentence_index": 43, "text": "Can this be useful for someone having less compute power and working on something similar to CelebA?", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}], "rebuttal_sentences": [{"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 0, "text": "We would like to thank Reviewer 2 for their review and constructive suggestions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 1, "text": "Our responses inline:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_global", null]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 2, "text": ">Discussions sometimes lack depth or are absent.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [15]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 3, "text": "-We have added an additional section (Appendix G) expanding on our discussion and providing additional insight into the observed instabilities.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [15]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 4, "text": ">For example, it is unclear to me why some larger models are not amenable to truncation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 5, "text": "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 6, "text": "-Truncation introduces a train-test disparity in G\u2019s inputs--at sampling time, G is given a distribution it has effectively never seen in training.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 7, "text": "The observation that imposing orthogonality constraints improves amenability to truncation is empirical.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 8, "text": "Our suspicion is that if G is not encouraged to be \u201csmooth\u201d in some sense, then it is likely that G will only properly generate images given points from the untruncated distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 9, "text": "We hypothesize that models which are not amenable end up learning mappings which, when given truncated noise, either attenuate or amplify certain activation pathways, leading to extreme output values (hence the observed saturation artifacts).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 10, "text": "We speculate that encouraging G\u2019s filters to have minimum pairwise cosine similarity means that, when exposed to distribution shift, the network\u2019s features are less correlated and less likely to align and amplify an activation path it would otherwise have learned to scale properly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [16, 17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 11, "text": ">Were samples from those networks better without using truncation? Why would this be?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 12, "text": "-Samples from those networks without truncation do not have measurably different quality, and their training metrics (losses, singular values) show no differences.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 13, "text": "Aside from empirically testing each network individually for amenability to truncation, we found no other way to check for that amenability.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 14, "text": "> Authors report how wider networks perform best, and how deeper networks degrade performance.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 15, "text": "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 16, "text": "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 17, "text": "-We are wary of explanations for which we do not have evidence.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 18, "text": "For each of the modifications introduced in Section 3, we offer a succinct conjecture as to why that change improves performance, but we are not aware of any existing reliable, informative metric which we could employ to understand or trace the source of each observed behavior, particularly with respect to GAN stability or performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 19, "text": "Regarding depth vs width: This paper is empirical, and we only briefly experimented with increasing depth analogously to increasing width.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 20, "text": "While increasing width provided an immediate measurable benefit, increasing depth did not.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 21, "text": "We felt that it was better to report the results of this brief investigation than to omit it for a lack of investigatory depth.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 22, "text": "> In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 23, "text": "Is this something the reader should understand from Table 1?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 24, "text": "-This means that of all the models we trained for the study presented in Table 1 which did not use Orthogonal Regularization, only 16% were amenable to truncation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 25, "text": "Of all the models which we trained for the study presented in Table 2 which did use Orthogonal Regularization, 60% were amenable to truncation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 26, "text": "This is not reflected in Table 1, which is merely a presentation of how the introduced modifications impact performance.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 27, "text": ">I question the choice of sections chosen to be in the main paper/appendices.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 28, "text": "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 29, "text": "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 30, "text": "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 31, "text": "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27, 28]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 32, "text": "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 33, "text": "-We appreciate this suggestion.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 34, "text": "While we recognize that this paper generally has a strong focus on implementation details, we felt that this instability was one of the most salient behaviors we observed, and that future work would be best served by presenting our investigations and attempts to understand its source, even if these methods did not improve performance.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 35, "text": "The information in Appendix B and C is intended to be of interest to those who want to reproduce our experiments, so it largely comprises hyperparameters and architectural details that we felt were not necessary to understand the main results of the paper.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [24, 25, 26, 27, 28, 29]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 36, "text": ">In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [36]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 37, "text": "-Thanks! This was indeed an error, which we\u2019ve corrected in the updated draft.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [36]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 38, "text": ">I would also be curious to see the proposed techniques applied on simpler datasets. Can this be useful for someone having less compute power and working on something similar to CelebA?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [42, 43]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 39, "text": "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42, 43]]}, {"review_id": "S1gaWerP2X", "rebuttal_id": "r1gI_-SQAm", "sentence_index": 40, "text": "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [42, 43]]}]}