{"metadata": {"forum_id": "S1lhbnRqF7", "review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "title": "Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension", "reviewer": "AnonReviewer3", "rating": 6, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=S1lhbnRqF7&noteId=r1e8lyS7RQ", "annotator": "anno2"}, "review_sentences": [{"review_id": "H1gOvMYT37", "sentence_index": 0, "text": "* Summary", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 1, "text": "This paper addresses machine reading tasks involving tracking the states of entities over text.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 2, "text": "To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 3, "text": "The paper reports positive evaluations on three different tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 4, "text": "* Review", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 5, "text": "This is an interesting paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "H1gOvMYT37", "sentence_index": 6, "text": "The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 7, "text": "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 8, "text": "This is especially the case in a few places involving coreference:", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 9, "text": "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 10, "text": "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "H1gOvMYT37", "sentence_index": 11, "text": "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 12, "text": "Why does the graph update require coreference pooling again?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "H1gOvMYT37", "sentence_index": 13, "text": "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 14, "text": "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1gOvMYT37", "sentence_index": 15, "text": "That the model implicitly learns constraints from data is interesting!", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1gOvMYT37", "sentence_index": 16, "text": "Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}], "rebuttal_sentences": [{"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 0, "text": "Thanks for the insightful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 1, "text": "We\u2019ve tried to improve our paper based on your feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 2, "text": "Most significantly, we\u2019ve performed additional ablation studies to confirm that our modeling choices improve performance, and we provide further empirical insight on what the coreference operations do.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 3, "text": "We\u2019ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 4, "text": "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 5, "text": "Below we address your concerns point-by-point.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 6, "text": "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 7, "text": "This is especially the case in a few places involving coreference:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 8, "text": "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 9, "text": "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 10, "text": "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 11, "text": "======", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 12, "text": "Based on your comments, we\u2019ve performed additional ablations to measure the impact of the co-reference mechanisms.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 13, "text": "We find that removing any of them leads to a decrease in performance (Rows 2, 3, 4 of Table 5).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 14, "text": "To provide more than just this quantitative insight, we\u2019ll expand here on how KG-MRC handles coreference to better motivate the modeling choices:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 15, "text": "The construction of graph G_t from G_{t-1} uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 16, "text": "We perform coreference disambiguation between location nodes of G_t and G_{t-1} via Eq. 1 (call this inter-graph coreference) and between the location nodes in the same graph Gt (call this intra-graph coreference) via Eq. 2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 17, "text": "The inter-graph coreference yields new, intermediate representations for the nodes in G_t.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 18, "text": "These are further updated via the intra-graph coreference step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 19, "text": "Inter-graph Co-ref: One way to think about this is that we construct a new graph G_t at every time step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 20, "text": "Now the graph G_{t-1} might contain some location nodes which are predicted again at time step \u2018t\u2019 (e.g., in Figure 2, leaf node already existed in G_{t-1}).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 21, "text": "Instead of replacing an old node with an entirely new node at \u2018t\u2019, we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018t\u2019.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 22, "text": "Intra-graph Co-ref: Inter-graph co-ref isn\u2019t enough since the MRC module makes its span predictions independently.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 23, "text": "This means that, at time step t, the model could predict the same span/location for multiple entities and add all these duplicates to the graph.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 24, "text": "Moreover, a single location might have the same surface form but be from different parts of the paragraph (e.g. \u201cleaf\u201d in the 1st and the 5th sentence of the para in figure 2).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 25, "text": "The operations in Eq. 2 resolve this by performing self-attention (i.e., the predicted locations of all entities are compared to each other).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7, 8, 9, 10, 11]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 26, "text": "=====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 27, "text": "Response continued from above.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 28, "text": "Why does the graph update require coreference pooling again?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 29, "text": "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 30, "text": "=====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 31, "text": "We agree that the coreference pooling in the graph update seems repetitive at first glance.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 32, "text": "We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 33, "text": "This step does indeed repeat Eq. 2.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 34, "text": "In a nutshell, this is necessary because, after the recurrent and residual graph updates (Eqs 3.1 - 3.3) that propagate information across edges, we may end up with different representations for location nodes corresponding to the same location.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 35, "text": "We don\u2019t want these representations to diverge from each other because of information propagation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 36, "text": "To give you more detail:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 37, "text": "The graph update step ensures information propagation between entities and location representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 38, "text": "Specifically if the current location of entity \u201ce_t\u201d is predicted as \u201c\\lambda_t\u201d, the graph update steps ensures that both the entity and location representation gets the same update (via eq 3.2 and 3.3).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 39, "text": "This would have been sufficient if every entity had a unique location.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 40, "text": "But, multiple entities can actually exist in the same location.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 41, "text": "Let\u2019s consider this small graph below", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 42, "text": "Water - -> leaf", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 43, "text": "CO_2 --> leaf", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 44, "text": "Here both water and CO_2 exist in the same location, leaf.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 45, "text": "But let\u2019s say that the MRC model picked the \u201cleaf\u201d span from sentence 1 (of the text in Fig 2) for \u201cWater\u201d and from sentence 4 for CO_2.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 46, "text": "In reality, they refer to the same location entity \u201cleaf\u201d.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 47, "text": "Now, due to eq. 3.3, the two embeddings of leaf will get two different residual updates (one would be corresponding to Water and other would be because of CO_2).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 48, "text": "Because of the different updates, the two representations of the same entity might diverge.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 49, "text": "To remedy this, we re-use the coreference matrix \u201cU\u201d we create in eq. (2), which should already have a high attention score corresponding to the two leaf locations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 50, "text": "Thus we perform a similar operation to the intra-graph update.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12, 13]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 51, "text": "====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 52, "text": "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 53, "text": "====", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 54, "text": "The \u201cprefixes\u201d that our model reads at each time step comprise all sentences up to and including the current sentence s_t.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 55, "text": "The motivation for this modeling choice was empirical.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 56, "text": "In our preliminary experiments we evaluated alternative strategies, such as (a) only considering the current sentence s_t, and (b) considering the entire paragraph at every time step.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 57, "text": "We found that operating on prefixes performed best.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}, {"review_id": "H1gOvMYT37", "rebuttal_id": "r1e8lyS7RQ", "sentence_index": 58, "text": "This is in line with the findings of Dalvi et al., 2018, where the Pro-Global model (which uses prefixes) performs better than the Pro-Local model (which operates on single sentences).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14]]}]}