{"metadata": {"forum_id": "HJlWWJSFDH", "review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "title": "Strategies for Pre-training Graph Neural Networks", "reviewer": "AnonReviewer2", "rating": 6, "conference": "ICLR2020", "permalink": "https://openreview.net/forum?id=HJlWWJSFDH&noteId=SJlhViuQsH", "annotator": "anno10"}, "review_sentences": [{"review_id": "BkxPCGTX9B", "sentence_index": 0, "text": "The authors introduce strategies for pre-training graph neural networks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 1, "text": "Pre-training is done at the node level as well as at the graph level.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 2, "text": "They evaluate their approaches on two domains, biology and chemistry on a number of downstream tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 3, "text": "They find that not all pre-training strategies work well and can in fact lead to negative transfer.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 4, "text": "However, they find that pre-training in general helps over non pre-training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 5, "text": "Overall, this paper was well written with useful illustrations and clear motivations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 6, "text": "The authors evaluate their models over a number of datasets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 7, "text": "Experimental construction and analysis also seems sound.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkxPCGTX9B", "sentence_index": 8, "text": "I would have liked to see a bit more analysis as to why some pre-training strategies work over others.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkxPCGTX9B", "sentence_index": 9, "text": "However, the authors mention that this is in their planned future work.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 10, "text": "Also, in figure 4, the authors mention that their pre-trained models tend to converge faster.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BkxPCGTX9B", "sentence_index": 11, "text": "However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.", "coarse": "arg_request", "fine": "arg-request_result", "asp": "asp_substance", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 0, "text": "We thank the reviewer for insightful feedback and for noting that our\u200b experiments \u200bare\u200b \u200bsolid\u200b and our setup and analyses are sound.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 1, "text": "The reviewer asks great questions, and we provide the answers below.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 2, "text": "RE: Total running time", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 3, "text": "The reviewer raises an interesting point about total training time, which includes the time to pre-train a GNN and the time to fine-tune it on a downstream task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 4, "text": "To address this point, below, we give the results of the total training time as well as the amortized total time over different downstream tasks.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 5, "text": "We will include detailed results and a discussion in the final version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr_manu_Yes", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 6, "text": "We note that although pre-training does take some time, it is a one-time-effort only.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 7, "text": "That is, we pre-train a GNN model only once and then reuse it many times by fine-tuning the model on any number of downstream prediction tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 8, "text": "Overall, we find that GNNs, once pre-trained, tend to converge much faster on downstream tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 9, "text": "Most importantly, we find (details below) that validation set performance converges 5-12 times more quickly when GNNs are pre-trained.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 10, "text": "We emphasize that this cannot be achieved by mere training of (non-pre-trained) GNNs longer.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 11, "text": "The following summarizes training time for chemistry and biology datasets.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 12, "text": "1) Chemistry dataset (single GPU implementation)", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 13, "text": "**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 14, "text": "Pre-training**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 15, "text": "\u2014 Self-supervised pre-training: 24 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 16, "text": "\u2014 Supervised pre-training: 11 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 17, "text": "**Fine-tuning on MUV dataset** [Time to achieve the best validation set AUC]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 18, "text": "\u2014 From random initialization (i.e., no pre-training):", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 19, "text": "1 hour; 74.9% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 20, "text": "\u2014 From a pre-trained GNN:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 21, "text": "5 minutes; 85.3% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 22, "text": "2) Biology dataset", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 23, "text": "**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 24, "text": "Pre-training**", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 25, "text": "\u2014 Self-supervised pre-training:  3.8 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 26, "text": "\u2014 Supervised pre-training: 2.5 hours", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 27, "text": "**Fine-tuning** [Time to achieve the best validation set AUC]", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 28, "text": "\u2014 From random initialization (i.e., no pre-training):", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 29, "text": "50 minutes; 84.8% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 30, "text": "\u2014 From a pre-trained GNN:", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 31, "text": "10 minutes; 88.8% AUC", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 32, "text": "On chemistry dataset, we see that fine-tuning a pre-trained GNN on the MUV required only 5 min.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 33, "text": "This is in sharp contrast with training a GNN from scratch, which required 12x more time, yet it gave a worse performance.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 34, "text": "We can reach similar conclusions on the biology dataset.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 35, "text": "We thus recommend using pre-trained models whenever possible as they can give better performance and can be reused for any number of downstream tasks.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 36, "text": "We shall add these results and explanations to the final version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr_manu_Yes", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 37, "text": "RE: Analysis of different pre-training strategies", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 38, "text": "Thank you for bringing up this valuable point.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 39, "text": "We agree that it is important to understand why some pre-training strategies work better over others.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 40, "text": "Our key insight backed up with extensive empirical evidence is that a combination of graph-level and node-level methods (Figure 1) is important because it allows the model to capture both local and global semantics of graphs.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkxPCGTX9B", "rebuttal_id": "SJlhViuQsH", "sentence_index": 41, "text": "Further, we find that our structure-based node-level methods (Context Prediction and Attribute Masking) are preferred over position-based node-level methods (Edge Prediction, Deep Graph Infomax). As future work, we plan to further investigate what graph-level and node-level methods are most useful in different domains, and understand what domain-specific knowledge has been learned by the pre-trained models.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [11]]}]}