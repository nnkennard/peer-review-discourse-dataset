{"metadata": {"forum_id": "B1lwSsC5KX", "review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "title": "D\u00e9j\u00e0 Vu: An Empirical Evaluation of the Memorization Properties of Convnets", "reviewer": "AnonReviewer2", "rating": 5, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=B1lwSsC5KX&noteId=HJeWT9TbCQ", "annotator": "anno3"}, "review_sentences": [{"review_id": "ByezUKSq2Q", "sentence_index": 0, "text": "Summary of the paper:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 1, "text": "The paper has two intertwined goals.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 2, "text": "These goals are to illuminate the generalization/memorization properties of large and deep ConvNets in tandem with trying to develop procedures related to identifying whether an input to a trained ConvNet has actually been used to train the network.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 7, "text": "The latter task is generalized to detecting if a particular dataset has been used to train a ConvNet. These goal are tackled empirically with multiple sets of experiments on largescale datasets such as ImageNet22k and modern deep ConvNets architectures such as VGG and ResNet.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 13, "text": "Paper's positive points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 14, "text": "+ The paper has a very comprehensive set of references in the areas it touches upon.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 16, "text": "+ Some of the experimental results presented are quite interesting. They show that regularization data-augmentation helps prevent a network from explicit memorization and could be used as a way to help make training data more anonymous.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 20, "text": "+ Large scale experiments are reported on modern architectures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 21, "text": "Paper's negative points", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 22, "text": "- The paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. If I'm not mistaken, from this result the authors extrapolate that the capacity of a (deep) neural network is proportional to the number of parameters in the network.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 30, "text": "This is true, but there are a couple of caveats.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 32, "text": "The first is that the coefficient of proportionality must depend very much on the number of layers in the network.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 35, "text": "Increasing the network's depth increases the efficiency of the representation (i.e. fewer total parameters needed to have the same representational power as a shallow network). And as MacKay also says in his book (chapter 44 quoting findings from Radford Neal) that for MLPs what determines the complexity of the typical function (once the network has a large enough width) represented by the MLP is the \"characteristic magnitude of the weights\". So the regularization technique applied is very significant in the controlling the effective capacity of a network.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 44, "text": "This paper experimentally shows that is the case multiple times as it is shown that with increasing degrees of regularization (figure 1, figure 2) it becomes harder and harder to memorize the positive training", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 48, "text": "images. It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 52, "text": "- There is a slight oxymoron in the premise of the first set of experiments. The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "ByezUKSq2Q", "sentence_index": 60, "text": "This issue is not really commented upon in the paper. Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 64, "text": "- This paper is a slightly difficult read - not because of the language or the presentation of the material but more because there is not one main coherent argument or goal for the paper. This is reflected in the \"Related work\" section where 4 different issues/tasks are referred to.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 70, "text": "Each one of these topics is worthy of a paper in itself, but this paper dips into each one and then swiftly moves onto the next one. For example in section 3 the paper explores if a network can be forced to explicitly memorize a set of images and how the size of this set is affected by the number of parameters in the network and data augmentation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 77, "text": "High-level conclusions are made: more parameters in the network implies more images can be memorized and data-augmentation makes explicit memorization more difficult. Then it is off to considering pre-trained networks and determining whether by analyzing the statistics of the responses at different layers one can decide if a set of images was used for training or not (or similar tasks). Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 89, "text": "- The conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author? If yes, would it better to re-organize the paper and devote more of it to the material presented in section 3 and filling this out with more analysis and experiments to perhaps explore the issue of the capacity of a network in more", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "ByezUKSq2Q", "sentence_index": 96, "text": "Queries/ points that need some clarification", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 97, "text": "- I'm a little unclear when data-augmentation is included in the training phase whether the goal is to be able to also recognise perturbed versions of the input images at test time. In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 104, "text": "- Last paragraph page 4: \"when the accuracy gets over 60\\% and again at 90\\%\". Is this training or validation accuracy?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 106, "text": "Typos possible errors spotted along the way:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "ByezUKSq2Q", "sentence_index": 107, "text": "* First paragraph page 5: \"more shallow\" --> \"shallower\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 108, "text": "* Page 7, first paragraph of section 5.: \"is ran\" --> \"is run\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "ByezUKSq2Q", "sentence_index": 109, "text": "* Using \"scenarii\" for the plural of \"scenario\" I would say is pretty non-standard and most people would use \"scenarios\"", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "asp_substance", "pol": "pol_neutral"}], "rebuttal_sentences": [{"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 0, "text": "We thank the reviewer for their review.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 1, "text": "\u201cThe paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. [...] It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 2, "text": "We agree with the reviewer that our analysis of capacity in section 3 does not take into account the magnitude of the weights, nor the dependence on the depth of the network.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 3, "text": "Our objective in this section was to provide a empirical lower-bound on the capacity by designing a setup where we can vary the quantity of information contained in a dataset (in our case, N choose n), and evaluate empirically the effect of data augmentation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 4, "text": "In relation to section 5, we aim at seeing how much a network can remember if it is explicitly trained to remember a given set of images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 5, "text": "We understand the limitations of MacKay's analysis, which was presented to give a rough theoretical comparison point to our empirical evaluation.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30, 32, 32, 32, 35]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 6, "text": "We will clarify this in the paper and improve the discussion along the lines discussed by the reviewer.", "coarse": "concur", "fine": "rebuttal_by-cr_manu_Yes", "alignment": ["context_sentences", [22, 22, 22, 22, 22, 22, 22, 22, 30, 30, 32, 32, 32, 35]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 7, "text": "\u201cThere is a slight oxymoron in the premise of the first set of experiments.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [52, 52]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 8, "text": "The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [52, 52]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 9, "text": "[...] Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 10, "text": "In these experiments, the set of positive and negatives is fixed (when varying data augmentation and architectures).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 11, "text": "During training, we feed to the network all positives and an equal number of negatives during each epoch.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 12, "text": "The performance does indeed depend on the closeness of the positive and the negatives, but this is similar to the membership inference problem presented in section 5, where it is difficult for a network to tell apart a seen image from an unseen, very similar image.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 13, "text": "A reconstruction task would suffer the same problems: the reconstruction is only approximate so we would need to evaluate the distance between our reconstruction, positives and negatives, which also depends on the closeness between positives and negatives.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 14, "text": "Also, the reconstruction task would need to remember the values of all pixels which requires more capacity.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 15, "text": "We agree that this specific deserves a short discussion and will add it to the paper.", "coarse": "concur", "fine": "rebuttal_by-cr_manu_Yes", "alignment": ["context_sentences", [60, 60, 60]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 16, "text": "\u201cThis paper is a slightly difficult read [...] because there is not one main coherent argument or goal for the paper.[...]. Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [64, 64, 64, 64, 64, 64, 70, 70, 70, 70, 70, 70, 70, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 17, "text": "The general goal of the paper is to empirically assess memorization in neural networks, and in particular the important question of implicit memorization, which is important for privacy: does a network trained for classification remember an image, or a set of images ?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [64, 64, 64, 64, 64, 64, 70, 70, 70, 70, 70, 70, 70, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 18, "text": "This aspect is empirically evaluated in sections 4 and 5, and section 3 is a preliminary study of the memorization capabilities for systems explicitly trained to memorize (this serves as a qualitative upper-bound for implicit memorization).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [64, 64, 64, 64, 64, 64, 70, 70, 70, 70, 70, 70, 70, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 19, "text": "We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 20, "text": "We decided to move it to an appendix after reading the feedback from the three reviewers.", "coarse": "concur", "fine": "rebuttal_done_manu_Yes", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 21, "text": "\u201cThe conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 22, "text": "We do not consider the conclusions of the experiments from Section 3  to be more important than those of the other sections, in fact quite the opposite.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 23, "text": "As mentioned above, we will move it to appendices.", "coarse": "concur", "fine": "rebuttal_by-cr_manu_Yes", "alignment": ["context_sentences", [89, 89, 89, 89, 89, 89, 89]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 24, "text": "\u201c[...]In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [97, 97, 97, 97, 97, 97, 97]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 25, "text": "When data augmentation is used, we consider that perturbed positive images are also positive images.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [97, 97, 97, 97, 97, 97, 97]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 26, "text": "In the testing phase, perturbed versions of the positive images are given to the ConvNet.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [97, 97, 97, 97, 97, 97, 97]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 27, "text": "\u201cLast paragraph page 4: \"when the accuracy gets over 60\\% and at 90\\%\". Is this training or validation accuracy?\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [104, 104]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 28, "text": "We decrease the learning rate when the training accuracy reaches these thresholds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [104, 104]]}, {"review_id": "ByezUKSq2Q", "rebuttal_id": "HJeWT9TbCQ", "sentence_index": 29, "text": "We thank the reviewer for reporting typos, we will correct them in the paper.", "coarse": "concur", "fine": "rebuttal_by-cr_manu_Yes", "alignment": ["context_sentences", [106, 107, 108, 109, 109]]}]}