{"metadata": {"forum_id": "H1lIzhC9FX", "review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "title": "Learning to remember: Dynamic Generative Memory for Continual Learning", "reviewer": "AnonReviewer3", "rating": 4, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=H1lIzhC9FX&noteId=S1laMLagAQ", "annotator": "anno14"}, "review_sentences": [{"review_id": "H1eAyvBa2Q", "sentence_index": 0, "text": "The proposed method tackles class-incremental continual learning, where new categories are incrementally exposed to the network but a classifier across all categories must be learned.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 1, "text": "The proposed method seems to be essentially a combination of generative replay (e.g. Deep Generative Replay) with AC-GAN as the model and attention (HAT), along with a growing mechanism to support saturating capacity.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 2, "text": "Quantitative results are shown on MNIST and SVHN while some analysis is provided on CIFAR.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 3, "text": "Pros", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 4, "text": "+ The method combines the existing works in a way that makes sense, specifically AC-GAN to support a single generator network with attention-based methods to prevent forgetting in the generator.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 5, "text": "+ The method results in good performance, although see caveats below.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 6, "text": "+ Analysis of the evolution of mask values over time is interesting.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 7, "text": "Cons", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 8, "text": "- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 9, "text": "The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of \"growing capacity\" is not made clear at all especially in the beginning of the paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 10, "text": "Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 11, "text": "The authors should on the claimed contributions.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 12, "text": "Is it a combination of DGR and HAT with some capacity expansion?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 13, "text": "- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 14, "text": "Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 15, "text": "- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 16, "text": "It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 17, "text": "- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 18, "text": "As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 19, "text": "It also seems strange to say that storing instances \"violates the strictly incremental setup\" while generative models do not.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 20, "text": "Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 21, "text": "Otherwise you are just defining the problem in a way that excludes other simple approaches which work.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 22, "text": "- There are several methodological issues: Why are CIFAR results not shown in a table as is done for the other dataset? How many times were the experiments run and what were the variances? How many parameters are used (since capacity can increase?) It is for example not clear that the comparison to joint training is fair, when stating: \"Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_replicability", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 23, "text": "This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations compared to what it would learn given all the data.\" Doesn't DGM grow the capacity, and therefore this isn't that surprising?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 25, "text": "This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 26, "text": "Some other minor issues in the writing includes:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 27, "text": "1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 28, "text": "The initial narrative mixes prior works' contributions and this paper's contributions; the contributions of the paper itself should be made clear,", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 29, "text": "2) Using the word \"task\" in describing \"joint training\" of the generative, discriminative, and classification networks is very confusing (since \"task\" is used for the continual learning description too,", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 30, "text": "3) There is no legend for CIFAR; what do the colors represent?", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 31, "text": "4) There are several typos/grammar issues e.g. \"believed to occurs\", \"important parameters sections\", \"capacity that if efficiently allocated\", etc.).", "coarse": "arg_request", "fine": "arg-request_typo", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 32, "text": "In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "H1eAyvBa2Q", "sentence_index": 33, "text": "More rigorous experiments and analysis is needed to make this a good ICLR paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 0, "text": "4.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 1, "text": "Our approach has 2 important hyperparameters: scaling parameter s used for calculating binary mask from the embedding matrix as well as  \u03bb_RU, that controls the size accuracy trade-off (see Sec. 4.1 \u201cjoint training\u201d).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 2, "text": "We add a table analyzing the sensitivity of the parameter \u03bb_RU observing the expected behavior: higher values of \u03bb_RU lead to a smaller model size, however, reduced G size is positively correlated with the final classification performance of D (smaller G -> lower accuracy of D).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 3, "text": "+---------+---------+-------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 4, "text": "| \u03bb_RU  | Acc.5 | Size |", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 5, "text": "+---------+---------+-------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 6, "text": "| 2E-06 | 98.16 | 660", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 7, "text": "|", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 8, "text": "+---------+--------+--------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 9, "text": "| 0.002 | 98.22 | 638", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 10, "text": "|", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 11, "text": "+---------+--------+--------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 12, "text": "| 0.2     | 98.02 | 598", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 13, "text": "|", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 14, "text": "+---------+--------+--------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 15, "text": "| 0.75   | 97.36 | 577", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 16, "text": "|", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 17, "text": "+---------+--------+--------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 18, "text": "| 2        | 86.82 | 522", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 19, "text": "|", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 20, "text": "+---------+--------+--------+", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [15, 16]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 21, "text": "5. We use the baseline presented by [1], that tackles identical scenario.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 22, "text": "To our knowledge [1] provides the state of the art performance in \"strict\" class incremental setup without using real samples.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 23, "text": "We consider a joint training (JT, classical training) of the discriminator as the upper performance bound.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 24, "text": "Joint training features a setup in which the discriminator is trained on ALL real samples of the previous tasks.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 25, "text": "The reviewer proposes to simulate information loss and use a random subset of real samples to train the upper bound model.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 26, "text": "However, this would certainly give a worse performance than when using all real samples.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 27, "text": "We, therefore, think that used JT upper bound is appropriate.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [17]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 28, "text": "Furthermore, using generated samples accommodates for better performance than simply storing instances only in case of tasks of relatively low complexity such as MNIST.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 29, "text": "Indeed, such a result has been shown in other works, e.g. [1].", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 30, "text": "As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with steady quality of the generated samples.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 31, "text": "Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 32, "text": "Thus, this effect can be observed neither in the SVHN nor the CIFAR10 benchmarks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [18]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 33, "text": "6. The CIFAR results will be provided in the Tab. 1 alongside with other datasets in the next version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [22]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 34, "text": "To ensure a fair comparison with the benchmark methods that do not use any network expansion strategy for the generator (e.g. [1,6]), we initialize our G to be approximately 50% of the size of the G used in the benchmarks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [25]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 35, "text": "Also a study on network growth dynamics is provided in Fig. 5 (Sec. 5.3), showcasing a lower network capacity than the worst case scenario.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [25]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 36, "text": "Growing the generator is an essential part of our method that addresses the scalability problem in continual learning, e.g. with always growing amount of data model\u2019s capacity will be exhausted at a certain point.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [25]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 37, "text": "Noteworthy, the discriminator is not affected by the proposed dynamic network expansion mechanism and features the same architecture as in the benchmark methods.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [25]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 38, "text": "We believe the comparison to the joint training is fair because DGM only grows the capacity of the generator.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 39, "text": "In the discriminator, only the last classification layer is expanded with the growing model\u2019s output space as new classes are added.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 40, "text": "Thus, for k-th task we compare the accuracy of a discriminator with identical architecture trained on real samples of all k tasks (JT) with one trained on DGM-synthesized samples of k-1 tasks+reals of k-th tasks.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 41, "text": "Thus DGM\u2019s discriminator has no advantages over the joint training generator.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [22]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 42, "text": "8. Finally, we will address typos, writing and presentation issues in the updated version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [31]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 43, "text": "[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 44, "text": "[2] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 45, "text": "URL http://arxiv.org/abs/1801.01423.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 46, "text": "[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 47, "text": "Continual learning with deep generative replay.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 48, "text": "In", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 49, "text": "Advances in Neural Information Processing Systems, pages 2990\u20132999, 2017.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 50, "text": "[4] S. Rebuffi, A. Kolesnikov, and C. H. Lampert. icarl: Incremental classifier and representation", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 51, "text": "learning.CoRR, abs/1611.07725, 2016. URL http://arxiv.org/abs/1611.07725.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 52, "text": "[5] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. CoRR, abs/1801.10112, 2018. URL http://arxiv.org/abs/1801.10112.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 53, "text": "[6] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990\u20132999, 2017.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 54, "text": "We thank the reviewer for their constructive comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 55, "text": "We address them as follows.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 56, "text": "1. We first would like to point out the contributions of our work.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 57, "text": "First, we address the catastrophic forgetting problem in continual learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 58, "text": "Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 59, "text": "Hereby we extend the idea of HAT[2] to generative networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 60, "text": "Secondly, we address the scalability problem in continual learning.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 61, "text": "To ensure sufficient model capacity to accommodate for new tasks, we propose an adaptive network expansion mechanism in which newly added capacity is derived from the learnable neuron masks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 62, "text": "2. We further we would like to clarify a possible confusion of the proposed method to be a combination of Deep Generative Replay (DGR)[6] and HAT[2].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 63, "text": "As pointed out in the Sec. 2 of our work, Deep Generative Replay (DGR) tries to prevent forgetting in the generator by retraining it from scratch every time a new data chunk becomes available.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 64, "text": "Thus, in DGR the generator would lose information at each replay step since the quality of generated samples highly depends on the quality of samples generated by the prior generator causing \"semantic drift\".", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 65, "text": "This contrasts our method, which effectively retains the knowledge in the generator using HAT like neuron masking and only loses information through \u201cnatural\u201d forgetting.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 66, "text": "This allows us to use \u201ccomplete\u201d learned representation during learning and inference of the subsequent tasks as well as speed up the training (no replay of G is involved).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 67, "text": "3. We are not simply shifting the forgetting problem into G.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 68, "text": "Our work tackles the problem of class incremental learning.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 69, "text": "As opposed to task-incremental setup and shown in previous work, e.g. [3,4,5], models in class incremental setup (with single-head architecture) require a replay of previously seen categories when learning new ones.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 70, "text": "The reason for using G", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 71, "text": "is not having access to samples of previous classes in the \u201cstrict\u201d incremental setup and using generated samples instead.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}, {"review_id": "H1eAyvBa2Q", "rebuttal_id": "S1laMLagAQ", "sentence_index": 72, "text": "As pointed out in our work, restricting storage of real samples represents a more realistic setup, since in real-world applications such an \u201cepisodic memory\u201d with real samples is often impossible due to memory and privacy restrictions.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [13, 14]]}]}