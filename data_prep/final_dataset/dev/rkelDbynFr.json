{"metadata": {"forum_id": "HJlQ96EtPr", "review_id": "rkelDbynFr", "rebuttal_id": "Bkev8dAwoS", "title": "FleXOR: Trainable Fractional Quantization", "reviewer": "AnonReviewer2", "rating": 6, "conference": "ICLR2020", "permalink": "https://openreview.net/forum?id=HJlQ96EtPr&noteId=Bkev8dAwoS", "annotator": "anno2"}, "review_sentences": [{"review_id": "rkelDbynFr", "sentence_index": 0, "text": "This paper proposed a fractional quantization method for deep net weights.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkelDbynFr", "sentence_index": 1, "text": "It adds XOR gates to produce quantized weight bits compared with existing quantization method.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkelDbynFr", "sentence_index": 2, "text": "It used tanh functions instead of a straight-through estimator for backpropagation during training.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkelDbynFr", "sentence_index": 3, "text": "With both designs, the proposed method outperformed the existing methods and offered sub bit quantization options.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "rkelDbynFr", "sentence_index": 4, "text": "The use of XOR gates to improve quantization seems novel.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "rkelDbynFr", "sentence_index": 5, "text": "The sub bit quantization achieved by this method should be interesting to the industrial.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "rkelDbynFr", "sentence_index": 6, "text": "It significantly improved the quantization rate with slightly quality degradation.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rkelDbynFr", "sentence_index": 7, "text": "With 1 bit quantization, it outperformed the state-of-the-art.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_positive"}, {"review_id": "rkelDbynFr", "sentence_index": 8, "text": "The results seem thorough and convincing.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}], "rebuttal_sentences": [{"review_id": "rkelDbynFr", "rebuttal_id": "Bkev8dAwoS", "sentence_index": 0, "text": "Response to AnonReviewer2", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkelDbynFr", "rebuttal_id": "Bkev8dAwoS", "sentence_index": 1, "text": "We would like to thank you for your positive review and comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "rkelDbynFr", "rebuttal_id": "Bkev8dAwoS", "sentence_index": 2, "text": "We would be happy if you have any other questions.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}]}