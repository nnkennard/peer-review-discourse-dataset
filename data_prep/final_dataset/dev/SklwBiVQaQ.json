{"metadata": {"forum_id": "SkeQniAqK7", "review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "title": "Combining Learned Representations for Combinatorial Optimization", "reviewer": "AnonReviewer4", "rating": 4, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=SkeQniAqK7&noteId=S1e9R9KOCQ", "annotator": "anno3"}, "review_sentences": [{"review_id": "SklwBiVQaQ", "sentence_index": 0, "text": "The paper proposes to combine several smaller, pretrained RBMs into a larger model as a way to solve combinatorial optimization problems.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 1, "text": "Results are presented on RBMs trained to implement binary addition, multiplication, and factorization, where the proposed approach is compared with the baseline of training a full model from scratch.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 2, "text": "I found the paper confusing at times.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 3, "text": "It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader. For instance, there\u2019s a brief exposition of the connection between Boltzmann machines and combinatorial optimization problems: the latter is mapped onto the former by expressing constraints as a fixed set of Boltzmann machine weights and biases, and low-energy states (i.e. more optimal solutions) are found by sampling from the model, which involves no training. What\u2019s less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 6, "text": "The paper states that the problem of training \"large modules\" is \"equivalent to solving the optimization problem\", but does not explain how.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 7, "text": "Similarly, the paper mentions that the \"general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem\", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 8, "text": "A concrete example is provided in the Experiments section: the authors propose to implement invertible (reversible?) boolean logic circuits by combining smaller pre-trained RBMs which implement certain logical operations into larger circuits.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 9, "text": "I have two issues with the chosen example: 1) the connection with combinatorial optimization is not clear to me, and 2) it\u2019s not very well explained.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 10, "text": "As far as I understand, these reversible boolean logic operations are expressed as sampling a subset of the RBM\u2019s inputs conditioned on another subset of its inputs.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 11, "text": "An example is presented in Figure 3 but is not expanded upon in the main text.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 12, "text": "I\u2019d like the authors to validate my understanding:", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 13, "text": "An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder\u2019s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 14, "text": "After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 15, "text": "The alternative to this, which is examined in the paper, is to train individual XOR, AND, and OR gates in the same way and compose them into a complete binary adder circuit as prescribed by Section 3.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 16, "text": "I think the paper has the potential to be a lot more transparent to the reader in explaining these concepts, which would avoid them spending quite a bit of time inferring meaning from figures.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SklwBiVQaQ", "sentence_index": 17, "text": "I\u2019m also confused by the presentation of the results. For instance, I don\u2019t know what \"log\", \"FA1\", \"FA2\", etc. refer to in Figure 6.", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 19, "text": "Also, Figure 6 is referenced in the text in the context of binary multiplication (\"[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6\"), but presents results for addition and factorization only.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SklwBiVQaQ", "sentence_index": 20, "text": "The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 21, "text": "I think there are interesting large-scale applications of this, such as building an autoregressive RBM for image generation by training a smaller RBM on a more restricted inpainting task.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "SklwBiVQaQ", "sentence_index": 22, "text": "The connection to combinatorial optimization, however, is much less clear to me.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 0, "text": "Thank you for your comments, we will be responding with specific comments to AnonReviewer4 here, and more general comments to the reviewer above.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 1, "text": "R4: \u201cWhat\u2019s less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [3]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 2, "text": "The combination method we propose here can be applied to RBMs that are calculated by directly setting weights and by training individual sub units.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 3, "text": "We acknowledge there are pros and cons to both approaches; directly calculating the weights gives guarantees on probabilities and mixing rates, while training can produce a more compact, data, and computationally efficient model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 4, "text": "Some algorithms will be more amenable to training, while others more amenable to directly calculating and setting weights, so we believe that this should be addressed on an algorithm by algorithm basis.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 5, "text": "We try to present one possible algorithm and a possible combination mechanism that we believe could work for others.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [3]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 6, "text": "R4: \u201cThe paper states that the problem of training \"large modules\" is \"equivalent to solving the optimization problem\", but does not explain how.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 7, "text": "Training a full module to solve an optimization problem in the context presented here involves supplying samples from a large portion of the subspace that we are trying to model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 8, "text": "Based on the results we have seen, we only achieve good performance once we have samples from >30% of the subspace (depending on the problem).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 9, "text": "In addition, the RBMs perform significantly better when trained on the full space we are trying to model.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 10, "text": "We view this as the RBM creating an associative memory where it \u201cmemorizes\u201d examples and recalls them afterward, and do not view this as a data and computationally efficient method of solving these problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 11, "text": "R4: An example is presented in Figure 3 but is not expanded upon in the main text.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 12, "text": "I\u2019d like the authors to validate my understanding:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11, 12]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 13, "text": "An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder\u2019s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11, 12, 13]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 14, "text": "After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.\u201d", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 9, 10, 11, 12, 13, 14]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 15, "text": "Yes, your understanding is correct.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 16, "text": "We train on the joint density over inputs and outputs, and solving a problem amounts to clamping (conditioning) a subset of the units and sampling the remaining units via Gibbs Sampling.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11, 12, 13, 14]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 17, "text": "We have made an effort in the revision to make sure that this is more clear.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9, 10, 11, 12, 13, 14]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 18, "text": "In the case of solving factorization problem, we clamp some of the visible units to the integer we are trying to factor, and use gibbs sampling to get statistics for the remaining units conditioned on the output number.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [9, 10, 11, 12, 13, 14]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 19, "text": "R4: I\u2019m also confused by the presentation of the results.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 20, "text": "For instance, I don\u2019t know what \"log\", \"FA1\", \"FA2\", etc. refer to in Figure 6.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 17]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 21, "text": "Also, Figure 6 is referenced in the text in the context of binary multiplication (\"[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6\"), but presents results for addition and factorization only.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 17, 19]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 22, "text": "We have presented results for addition and factorization in the main body of the paper, but refer to readers of the paper to the appendix where we have included a larger set of results.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 17, 19]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 23, "text": "The results were omitted from the main body of the paper for the sake of brevity.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [17, 17, 19]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 24, "text": "The naming of units as \u201clog\u201d \u201cFA1\u201d, \u201cFA2\u201d, etc. are meant to represent the size of the base unit that was merged to create this larger unit, \u201clog\u201d referring to logical units (AND, XOR, etc.), \u201cFA1\u201d being 1 bit full adder, \u201cFA2\u201d being a 2 bit full adder, etc. we have made this clear in the figure caption.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 17, 19]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 25, "text": "R4: The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 26, "text": "We also agree that there may be other applications to this type of merging of RBMs without further training, and we are working to look at those in greater detail.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [20]]}, {"review_id": "SklwBiVQaQ", "rebuttal_id": "S1e9R9KOCQ", "sentence_index": 27, "text": "Invertible Boolean Logic provides a good test bed for this idea, and as explained above, we do believe it has a very intimate relationship with Boolean Satisfiability problems and other combinatorial optimization problems.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [20]]}]}