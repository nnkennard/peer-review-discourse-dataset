{"metadata": {"forum_id": "B1gXWCVtvr", "review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "title": "Adapting Behaviour for Learning Progress", "reviewer": "AnonReviewer1", "rating": 3, "conference": "ICLR2020", "permalink": "https://openreview.net/forum?id=B1gXWCVtvr&noteId=SklrWZHuoS", "annotator": "anno3"}, "review_sentences": [{"review_id": "r1eP2ufv9r", "sentence_index": 0, "text": "This paper develops a multi-arm bandit-based algorithm to dynamically adapt the exploration policy for reinforcement learning.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eP2ufv9r", "sentence_index": 1, "text": "The arms of the bandit are parameters of the policy such as exploration noise, per-action biases etc.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eP2ufv9r", "sentence_index": 2, "text": "A proxy fitness metric is defined that measures the return of the trajectories upon perturbations of the policy z; the bandit then samples perturbations z that are better than the average fitness of the past few perturbations.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "r1eP2ufv9r", "sentence_index": 3, "text": "I think this paper is just below the acceptance threshold.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "r1eP2ufv9r", "sentence_index": 4, "text": "My reservations and comments are as follows.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1eP2ufv9r", "sentence_index": 5, "text": "1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments. For instance, the authors use Rainbow as  the base algorithm upon which they add on the exploration. Rainbow itself is an extremely complicated algorithm, how can one be certain that the improvements in performance are caused by the improved exploration and not a combination of the bandit\u2019s actions with the specifics of Rainbow?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1eP2ufv9r", "sentence_index": 8, "text": "2. I don\u2019t understand Figure 4. The score defined in Appendix is the average over games for which seed performs better.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1eP2ufv9r", "sentence_index": 10, "text": "Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s\u2019 are two values of the arm in Figure 4? If not, how should one interpret Figure 4, no fixed arm is always good because the performance varies across the seeds. The curated bandit does not seem to be doing any better than a fixed arm.", "coarse": "arg_request", "fine": "arg-request_clarification", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1eP2ufv9r", "sentence_index": 13, "text": "I have a few more questions that I would like the authors to address in their rebuttal or the paper.", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "r1eP2ufv9r", "sentence_index": 14, "text": "1. The proxy f(z) does not bear any resemblance to LP(z). Why discuss the LP(z) then. The way f(z) is defined, it is just the value function averaged over perturbations  of the policy. If one were to consider z as an additional action space that is available to the agent during exploration, f(z) is the value function itself. The exploration policy is chosen not to maximize the E_z [f(z)] directly but to maximize the lower bound in Markov\u2019s inequality (P(f(z) >= t) <= E_z [f(z)]/t) in Section 4.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1eP2ufv9r", "sentence_index": 19, "text": "2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "r1eP2ufv9r", "sentence_index": 20, "text": "3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters. In this aspect, the auto-tuner for exploration is a plug-and-play procedure in other RL algorithms.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "r1eP2ufv9r", "sentence_index": 22, "text": "4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies. What is the benefit of the added complexity?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 0, "text": "Thank you for your constructive feedback!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 1, "text": "Main comment 1:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 2, "text": "Absolutely, this is a difficult issue: there is no perfect middle ground where it is possible to study the contributions in their simplest instantiations while at the same time verifying their practical effectiveness.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 3, "text": "We have opted to place the bulk of our emphasis on a realistic scenario (Atari with a Rainbow-like learning agent) that practitioners of Deep RL would find relevant.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 4, "text": "To isolate effects, our experimental section includes many variants and ablations, allowing us to state with confidence that modulating behaviour using the bandit improves performance compared to uniform (no bandit) or untuned (fixed modulation) baselines.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 5, "text": "And this is separately validated across multiple classes of modulations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 6, "text": "But indeed, as you point out, we cannot guarantee that the improvements we see are purely due to exploration.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 7, "text": "At the same time, it\u2019s worth recognising that, by design, the method proposed will try to cater to the underlying learning algorithm and would ideally generate samples that would benefit the underlying learning procedure.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 8, "text": "We will highlight this ambiguity in the revised paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [5, 5, 5]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 9, "text": "Main comment 2:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 10, "text": "Sorry, this was not very clear: The performance outcome for each variant is measured on multiple independent runs (seeds).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 11, "text": "All outcomes are then jointly ranked, and the corresponding ranks are averaged across seeds.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 12, "text": "Finally, these averaged ranks are normalized to fall between 0 and 1.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 13, "text": "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 14, "text": "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 15, "text": "Note that these joining rankings are done separately per subplot (ie modulation class).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 16, "text": "Thus the reason that no fixed arm is always good does not depend on the inter-seed variability as much as on the fact that the best arm differs in different games.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 17, "text": "We will clarify this in the caption too.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 18, "text": "The bandit does not generally do better than the best fixed arm in hindsight -- in general, this would still need to be identified --", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 19, "text": "but it is not far off, and it handily outperforms untuned arms, allowing us to remove some of the hyper-parameter tuning burden.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10, 10, 10]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 20, "text": "Additional question 1:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14, 14, 14, 14, 14]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 21, "text": "We acknowledge that our presentation focused maybe more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [14, 14, 14, 14, 14]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 22, "text": "The updated paper will change the emphasis, and clarify that a closer, more faithful, learning progress proxy remains future work.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [14, 14, 14, 14, 14]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 23, "text": "We will also clarify that the little phrase \u201cAfter initial experimentation, we opted for the simple proxy\u2026\u201d implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [14, 14, 14, 14, 14]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 24, "text": "Additional question 2:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [19]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 25, "text": "Of course, even an ideal metric LP(z) would remain a local quantity, and pursuing it would not guarantee the maximal final performance -- but it is valuable if local optima are not the prime concern.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 26, "text": "Performance plateaus are a nuisance in general, and within the simple space of modulations we consider, there is no magic bullet to escape them.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 27, "text": "However, our approach does the next best thing: when performance becomes an uninformative (ie on a plateau), it encourages maximal diversity of behaviour (tending toward uniform probabilities over z), with the hope that some modulation gets lucky -- and then as soon as that happens, very quickly focusing on that modulation to repeat the lucky episode until learning is progressing again.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [19]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 28, "text": "Additional question 3:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [20, 20]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 29, "text": "Indeed, thank you. We have updated the text to place more emphasis on this contribution.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [20, 20]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 30, "text": "Additional question 4:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 31, "text": "The way we would summarize these results is that the bandit is more or less on par with the *best* fixed exploration policy, and so the added complexity is justified by reducing the need to tune exploration. Is this what you meant?", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [22, 22]]}, {"review_id": "r1eP2ufv9r", "rebuttal_id": "SklrWZHuoS", "sentence_index": 32, "text": "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}]}