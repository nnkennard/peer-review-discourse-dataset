{"metadata": {"forum_id": "Hye64hA9tm", "review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations", "reviewer": "AnonReviewer3", "rating": 4, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=Hye64hA9tm&noteId=BkgH9OTSAQ", "annotator": "anno12"}, "review_sentences": [{"review_id": "BklIRmpxa7", "sentence_index": 0, "text": "This paper tries to quantify how \"dense\" representations we need for a specific task -- more specifically, how many dimensions are needed from a given representation (for a given task) to achieve a percentage of the performance of the entire representation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 1, "text": "The second thing the paper tries to quantify is how well representations learned for one task can be fine tuned for another.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 2, "text": "Experiments are conducted with 4 different representation technique on a dozen or so tasks.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 3, "text": "Quick summary: While I liked aspects of this -- including the motivation of having a lightweight way of understanding how well representations transfer across tasks, overall my concerns surrounding the methodology and some missing analysis leads me to believe this needs more work before it is ready for publication.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BklIRmpxa7", "sentence_index": 4, "text": "Quality: Below average", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 5, "text": "I believe the proposed techniques have some flaws which hurt the eventual method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 6, "text": "There are also concerns about the motivations behind parts of the technique.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 7, "text": "Clarity: Fair", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_neutral"}, {"review_id": "BklIRmpxa7", "sentence_index": 8, "text": "There were some experimental details that were poorly explained but in general the paper was readable.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 9, "text": "Originality: Fair", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_neutral"}, {"review_id": "BklIRmpxa7", "sentence_index": 10, "text": "There were some nice ideas in the work but I remain concerned about aspects of it.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_neutral"}, {"review_id": "BklIRmpxa7", "sentence_index": 11, "text": "Significance: Below average", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 12, "text": "My concern is that the flaws in the method do not make it conducive to use as is.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 13, "text": "Strengths / Things I liked:", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 14, "text": "+ I really liked the motivating problem of being able to (hopefully cheaply / efficiently) estimate transfer potential to understand how well representations will perform on a different task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BklIRmpxa7", "sentence_index": 15, "text": "+ Multiple representations and tasks experimented with", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BklIRmpxa7", "sentence_index": 16, "text": "Weaknesses / Things that concerned me:", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 17, "text": "(In no specific order)", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 18, "text": "- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 19, "text": "While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 20, "text": "This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 21, "text": "Let's take an example: Say there is a single dimension of the representation that is a perfect predictor of a task.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 22, "text": "Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 23, "text": "To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 24, "text": "- (W2) Related to the last line: I did not see any experiments / analysis showing how stable these different numbers are across different runs of the representation technique. Nor did I see any error bars in the experiments.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 25, "text": "This again greatly concerned me as I am not certain how stable these metrics are.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 26, "text": "- (W3) Baselines for transfer learning: I felt this was another notable oversight.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 27, "text": "I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 28, "text": "This latter baseline is a zero-cost baseline as it is not even dependent on the method.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 29, "text": "- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how \"precision\" and NDCG are used as metrics.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 30, "text": "Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the \"gold\" set. How is precision and NDCG calculated from this?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 31, "text": "More importantly I don't believe looking at rank alone is sufficient since that completely obscures the actual performance numbers obtained via transfer.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_neutral"}, {"review_id": "BklIRmpxa7", "sentence_index": 32, "text": "In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 33, "text": "- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 34, "text": "(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 35, "text": "- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 36, "text": "-", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 37, "text": "(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 38, "text": "I find this striking because I can easily come up with cheaper alternatives to get at this \"density\".", "coarse": "arg_evaluative", "fine": "none", "asp": "arg_other", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 39, "text": "For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 40, "text": "If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 41, "text": "Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_meaningful-comparison", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 42, "text": "-", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 43, "text": "(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 44, "text": "- (W8)", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "BklIRmpxa7", "sentence_index": 45, "text": "The proposed  CLF weight difference method has some concerning aspects as well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 46, "text": "For example say we had two task with exact opposite labels.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 47, "text": "They would have a very low weight difference score though they are ideal representations for each other.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "BklIRmpxa7", "sentence_index": 48, "text": "Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 0, "text": "We thank the reviewer for their insightful and constructive feedback.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 1, "text": "Re: (W1 & W2) Adversely affected by rotations", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21, 22, 23, 24, 25]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 2, "text": ">", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_none", null]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 3, "text": "While the CFS is not invariant to rotations, it is a surprising, and empirically noteworthy, finding that all 4 different ways of producing representations consistently encode a dozen tasks very succinctly.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21, 22, 23, 24, 25]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 4, "text": "This is in line with some early work that observed that certain characteristic properties like length [1][2], sentiment [3], presence/absence of brackets [4] are encoded in a single dimension in the space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21, 22, 23, 24, 25]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 5, "text": "Some of these findings can be attributed to the additive property of the cell state of the LSTMs c_t = f_t c_{t-1} + h_t {c\u2019}_{t}, which is free from matrix rotations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21, 22, 23, 24, 25]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 6, "text": "As previously noted, this empirically also results in single cells of the LSTM being interpretable.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21, 22, 23, 24, 25]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 7, "text": "To just give one example, LSTM cell state can increment by a fixed amount at every time step and can count the number of tokens reliably (i.e the string length) [1][4].", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [18, 19, 20, 21, 22, 23, 24, 25]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 8, "text": "Re: (W3) Baselines for transfer learning:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 9, "text": "> The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 10, "text": "We can clearly see that the random ordering is much worse compared to informed metrics that use representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 11, "text": "Upon your suggestion, we would also add this random baseline in table 2 as well.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [26, 27, 28]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 12, "text": "Re: (W4) Metrics for ranking of transfer don't make sense (and some are missing).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 13, "text": "How is precision and NDCG calculated", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 14, "text": "> To compute the gold set, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 15, "text": "The ranked list (in the decreasing utility of transfer learning gain) is then considered the \u2018gold\u2019 set.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 16, "text": "We further compare our recommendations of candidate tasks generated using CFS and classifier weight difference methods against the gold ranked list.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 17, "text": "Precision@K, Reciprocal Rank and NDCG are among the popular information retrieval metrics to compare a recommended list against a gold ranked list.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 18, "text": "These metrics are meaningful in our case, for instance, Reciprocal Rank tells us how many tasks we need to consider as per our recommendation before we hit the highest performing candidate task.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 19, "text": "Figure 3 presents the accuracy boost using the best task till now in the produced recommendations for the candidate tasks using different methods.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 20, "text": "Regarding missing values:", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 21, "text": "As we explain in the paper, classifier weight difference metric is only applicable in cases", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 22, "text": "where the number of features between the tasks are of the same size.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 23, "text": "Thus, 2 sentence input tasks and 1 sentence input tasks cannot be compared using the metric.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 30, 31, 32]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 24, "text": "Re: (W5) Multi-task learning", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [33, 34]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 25, "text": "> Our goal is somewhat orthogonal to the multitask learning setting where all the tasks are jointly trained.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [33, 34]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 26, "text": "We, instead, focus on how the task-specific information is present in popular sentence representations, and how this could be used to assess transfer potential among tasks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [33, 34]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 27, "text": "Re: (W6) Motivation for CFS", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 28, "text": "> There is a rich literature concerning what information is captured in the representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 29, "text": "Further, there are a few initial works that show that certain characteristics like length [1][2], sentiment [3], presence and absence of tokens like brackets [4] are densely captured in a single dimension of the representation space.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 30, "text": "In a similar spirit, we wanted to quantitatively study this surprising phenomenon, and we were curious about how densely is information encoded in representations.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [35]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 31, "text": "Re: (W7) Alternatives to CFS / Computational concerns", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [37, 38, 39, 40, 41, 43]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 32, "text": "> We agree that LARS/LASSO could act as potential ways to attain reduced dimensions.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [37, 38, 39, 40, 41, 43]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 33, "text": "For our use case, we found the greedy forward selection computationally fast enough (of the order of a few minutes), and we observed a significant portion of accuracy captured in a very few dimensions.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [37, 38, 39, 40, 41, 43]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 34, "text": "We would definitely explore this further, and add a detailed analysis on computational efficiency of different methods to reduce dimensions.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [37, 38, 39, 40, 41, 43]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 35, "text": "Re: (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [44, 45, 46, 47, 48]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 36, "text": "They would have a very low weight difference score though they are ideal representations for each other", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [44, 45, 46, 47, 48]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 37, "text": "> You are right. For the very same reason, we take the inverse of the difference of normalized absolute classifier weights (Section 4.2).", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [44, 45, 46, 47, 48]]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 38, "text": "References:", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 39, "text": "1.\u201cWhy Neural Translations are the Right Length\u201d :http://www.aclweb.org/anthology/D16-1248.pdf", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 40, "text": "2. On the Practical Computational Power of Finite Precision RNNs for Language Recognition: https://arxiv.org/abs/1805.04908", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 41, "text": "3. Learning to Generate Reviews and Discovering Sentiment: https://arxiv.org/abs/1704.01444", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}, {"review_id": "BklIRmpxa7", "rebuttal_id": "BkgH9OTSAQ", "sentence_index": 42, "text": "4. Visualizing and Understanding Recurrent Networks : https://arxiv.org/abs/1506.02078", "coarse": "nonarg", "fine": "rebuttal_other", "alignment": ["context_global", null]}]}