{"metadata": {"forum_id": "r1lohoCqY7", "review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "title": "Learning-Based Frequency Estimation Algorithms", "reviewer": "AnonReviewer1", "rating": 6, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=r1lohoCqY7&noteId=Skl8UhSqaX", "annotator": "anno3"}, "review_sentences": [{"review_id": "BkeVYzOCnm", "sentence_index": 0, "text": "This paper introduces the study of the problem of frequency estimation algorithms with machine learning advice.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeVYzOCnm", "sentence_index": 1, "text": "The problem considered is the standard frequency estimation problem in data streams where the goal is to estimate the frequency of the i-th item up to an additive error, i.e. the |\\tilde f_i - f_i| should be minimized where \\tilde f_i is the estimate of the true frequency f_i.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "BkeVYzOCnm", "sentence_index": 2, "text": "Pros:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeVYzOCnm", "sentence_index": 3, "text": "-- Interesting topic of using machine learned advice to speed up frequency estimation is considered", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "BkeVYzOCnm", "sentence_index": 4, "text": "-- New rigorous bounds are given on the complexity of frequency estimation under Zipfian distribution using machine learned advice", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_positive"}, {"review_id": "BkeVYzOCnm", "sentence_index": 5, "text": "-- Experiments are given to justify claimed improvements in performance", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "BkeVYzOCnm", "sentence_index": 6, "text": "Cons:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeVYzOCnm", "sentence_index": 7, "text": "-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "BkeVYzOCnm", "sentence_index": 8, "text": "-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i\u2019s themselves. While in some applications this might be natural, this is certainly very restrictive in situations where f_i\u2019s are updated not just by +/-1 increments but through arbitrary +/-Delta updates, as in this case it might be more natural to assume that the distribution of the queries might be proportional to the frequency that the corresponding coordinate is being updated, for example.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeVYzOCnm", "sentence_index": 10, "text": "-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeVYzOCnm", "sentence_index": 11, "text": "-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher\u201918.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "BkeVYzOCnm", "sentence_index": 12, "text": "-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "BkeVYzOCnm", "sentence_index": 13, "text": "Other comments:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "BkeVYzOCnm", "sentence_index": 14, "text": "-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_substance", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 0, "text": "Thank you for the thoughtful comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 1, "text": "We are glad that you found our topic interesting and appreciated our theoretical analysis and experimental results.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [3]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 2, "text": "We address other comments below:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 3, "text": "[Results are only given for the Zipfian distribution]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 4, "text": "Many real-world data naturally follow the Zipf\u2019s Law, as we showed in Figure 5.1 and Figure 5.3 for internet traffic and search query data.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 5, "text": "Thus, our theoretical analysis assumes item frequencies follow the Zipfian distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 6, "text": "While our analysis makes this assumption, our algorithm does not have any assumption on the frequency distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [7]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 7, "text": "[Assuming query distribution is the same as data distribution]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 8, "text": "As the reviewer pointed out, the query distribution we use is a natural choice.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 9, "text": "There might be other types of query distributions, such as the one pointed out by the reviewer.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 10, "text": "Intuitively, our overall approach that separates heavy hitters from the rest should still be beneficial to such query distribution.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [8, 8, 10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 11, "text": "[Algorithm design]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 12, "text": "We agree that our algorithms are relatively simple.", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 13, "text": "We believe this is a feature not a bug: as we showed in Sec. 4.1, our algorithm does not need to be more complex.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 14, "text": "Specifically, our Learned Count-Min algorithm achieves the same asymptotic error as the \u201cIdeal Count-Min\u201d, which is allowed to optimize the whole hash function for the specific given input (Theorem 7.14 and Theorem 8.4 in Table 4.1).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 15, "text": "The proof of this statement demonstrates that identifying heavy hitters and placing them in unique bins is an (asymptotically) optimal strategy.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 16, "text": "(In fact, our first attempt at solving the problem was a much more complex algorithm which optimized the allocation of elements to the buckets (i.e., the whole hash function h) to minimize the error.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 17, "text": "This turned out to be unnecessary, as per the above argument.)", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 18, "text": "[Novelty compared to Mitzenmacher\u2019 18]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 19, "text": "Our paper, as well as the works of Kraska et al \u201918, Mitzenmacher \u201918,  Lykouris &", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 20, "text": "Vassilvitskii \u201918, Purohit et al, NIPS\u201918, belong to a growing class of studies that use a machine learning oracle to improve the performance of algorithms.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 21, "text": "All such papers use a learned oracle of some form.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 22, "text": "The key differences are in what the oracle does, how it is used, and what can be proved about it.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 23, "text": "In Kraska\u201918 and Mitzenmacher\u201918, the oracle tries to directly solve the main problem, which is: \u201cis the element in the set?\u201d An analogous approach in our case would be to train an oracle that directly outputs the frequency of each element.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 24, "text": "However, instead of trying to directly solve the main problem (estimate the frequency of each element), our oracle is a subroutine that tries to predict the best resource allocation --i.e., it tries to answer the question of which elements should be given their own buckets and which should share with others.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 25, "text": "There are other differences.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 26, "text": "For example, the main goal of our algorithm is to reduce collisions between heavy items, as such collisions greatly increase errors.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 27, "text": "This motivates our design to split heavy and light items using the learned model, and apply separate algorithms for each type.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 28, "text": "In contrast, in existence indices, all collisions count equally.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 29, "text": "Finally, our theoretical analysis is different from M'18 due to the intrinsic differences between the two problems, as outlined in the previous paragraph.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [11]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 30, "text": "[The analysis is relatively straightforward]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [12]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 31, "text": "There are three main theorems in our paper: Theorem 8.4, Theorem 7.11 and 7.14.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 32, "text": "Our proofs of Theorem 7.11 and 7.14 are technically involved, even if the techniques are relatively standard.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 33, "text": "On the other hand, the proof of Theorem 8.4 uses entirely different techniques.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 34, "text": "In particular, it provides a characterization of the hash function optimized for a particular input.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [12]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 35, "text": "[The machine learned Oracle is assumed to be flawless at identifying the Heavy Hitters]", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [14]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 36, "text": "Actually, this is not the case.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 37, "text": "The analysis in the paper already takes into account errors in the machine learning oracle.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 38, "text": "Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 39, "text": "In summary, our results hold even if the learned oracle makes prediction errors with probability O(1/ln(n)).", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [14]]}, {"review_id": "BkeVYzOCnm", "rebuttal_id": "Skl8UhSqaX", "sentence_index": 40, "text": "We will revise the text to make it clearer.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [14]]}]}