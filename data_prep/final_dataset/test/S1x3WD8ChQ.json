{"metadata": {"forum_id": "HkMlGnC9KQ", "review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "title": "On Regularization and Robustness of Deep Neural Networks", "reviewer": "AnonReviewer1", "rating": 5, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=HkMlGnC9KQ&noteId=Skx8HkyGT7", "annotator": "anno10"}, "review_sentences": [{"review_id": "S1x3WD8ChQ", "sentence_index": 0, "text": "Regularizing RKHS norm is a classic way to prevent overfitting.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 1, "text": "The authors note the connections between RKHS norm and several common regularization and robustness enhancement techniques, including gradient penalty, robust optimization via PGD and spectral norm normalization.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 5, "text": "They can be seen as upper or lower bounds of the RKHS norm.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 7, "text": "There are some interesting findings in the experiments. For example, for improving generalization, using the gradient penalty based method seems to work best.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 10, "text": "For improving robustness, adversarial training with PGD has the best results (which matches the conclusions by Madry et al.); but as shown in Figure 2, because adversarial training only decreases a lower bound of RKHS norm, it does not necessarily decrease the upper bound (the product of spectral norms).", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 15, "text": "This can be shown as a weakness of adversarial training if the authors explore further and deeper in this direction.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 17, "text": "Overall, this paper has many interesting results, but its contribution is limited because:", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 19, "text": "1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has been well studied by previous literature. This paper simply applies these results to deep neural networks, by treating the neural network as a big black-box function f(x) .", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 25, "text": "Many of the results have been already presented in previous works like Bietti & Mairal (2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 27, "text": "2. In experiments, the authors explored many existing methods on improving generalization and robustness. However all these methods are known and not new.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 29, "text": "Ideally, the authors can go further and propose a new regularization method based on the connection between neural networks and RKHS, and conduct experiments to show its effectiveness.", "coarse": "arg_other", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 32, "text": "The paper is overall well written, and the introductions to RKHS and each regularization techniques are very clear.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_positive"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 34, "text": "The provided experiments also include some interesting findings.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "S1x3WD8ChQ", "sentence_index": 36, "text": "My major concern is the lack of novel contributions in this paper.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_originality", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 0, "text": "We thank the reviewer for his comments.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 1, "text": "We address the comments about novelty in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ), for instance concerning the relationship to previous work, and the regularization penalty ||f||_M we propose.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [17, 17, 19, 19, 19, 19, 19, 25, 25, 27, 27, 29, 29, 29, 36]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 2, "text": "More detailed comments are addressed below.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 17, 19, 19, 19, 19, 19, 25, 25, 27, 27, 29, 29, 29, 36]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 3, "text": "**", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 4, "text": "weakness of adversarial training", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [10, 10, 10, 10, 15, 15]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 5, "text": "As noted in our general response, our ||f||_M regularization approach empirically yields models with a more useful certified generalization guarantee in the presence of adversaries on Cifar10, while PGD adversarial training would likely require local verification of robustness around each test example, and we are not aware of useful guarantees on adversarial generalization for such models.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [10, 10, 10, 10, 15, 15]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 6, "text": "We agree that this aspect is not clear in the current submission, and we will improve it in the next version.", "coarse": "concur", "fine": "rebuttal_future", "alignment": ["context_sentences", [10, 10, 10, 10, 15, 15]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 7, "text": "*", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_in-rebuttal", null]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 8, "text": "* relationship with traditional RKHS regularization", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 29, 29]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 9, "text": "There is indeed no question that kernel methods/RKHSs have been widely used for regularization of non-linear functions, for over 20 years now, however these methods typically rely on solving convex optimization problems using the kernel trick, or various kernel approximations (such as random Fourier features).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 29, 29]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 10, "text": "Separately, defining RKHSs that contain neural networks has indeed been the study of previous work, such as Bietti and Mairal (2018) or Zhang et al. (2016; 2017), however these only study theoretical properties of the kernel mapping and the RKHS norm, or derive convex learning procedures to replace training neural networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 29, 29]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 11, "text": "Our approach is quite different, in that we leverage these insights to obtain practical regularization strategies for generic neural networks.", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 29, 29]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 12, "text": "** new regularization methods", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [29, 29, 29]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 13, "text": "In addition to the ||f||_M lower bound penalty discussed in our general response, we note that combined approaches based on lower bound + upper bound methods are also novel to the best of our knowledge, and in particular we found combining robust optimization techniques with spectral norm constraints to be quite successful in many of the small data scenarios considered (see Table 1).", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [29, 29, 29]]}, {"review_id": "S1x3WD8ChQ", "rebuttal_id": "Skx8HkyGT7", "sentence_index": 14, "text": "We will happily clarify some of these points in an updated version of the paper.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [29, 29, 29]]}]}