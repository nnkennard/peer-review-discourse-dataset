{"metadata": {"forum_id": "rJlqoTEtDB", "review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "title": "PowerSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization", "reviewer": "AnonReviewer3", "rating": 8, "conference": "ICLR2020", "permalink": "https://openreview.net/forum?id=rJlqoTEtDB&noteId=BJgxTZgSir", "annotator": "anno10"}, "review_sentences": [{"review_id": "S1gTSur19S", "sentence_index": 0, "text": "This paper proposes, analyzes, and empirically evaluates PowerSGD (and a version with momentum), a simple adjustment to standard SGD algorithms that alleviates issues caused by poorly scaled gradients in SGD.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 1, "text": "The rates in the theoretical analysis are competitive with those for standard SGD, and the empirical results argue that PowerSGD algorithms are competitive with widely used adaptive methods such as Adam and RMSProp, suggesting that PowerSGD may be a useful addition to the armory of adaptive SGD algorithms.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 2, "text": "Overall I recommend acceptance of this paper, although I think there may be a couple of places where the authors overclaim a bit on the theoretical side.", "coarse": "arg_social", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 3, "text": "Specifically:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 4, "text": "\u2022", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 5, "text": "The convergence analysis assumes a batch size equal to T, the number of steps of PowerSGD.", "coarse": "arg_fact", "fine": "none", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 6, "text": "This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 7, "text": "If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 8, "text": "\u2022", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 9, "text": "In Remark 3.4.3, the authors claim that another point of difference between their results and Yan et al.'s (2018) is that Yan et al. assume bounded gradients, an assumption that is not satisfied for e.g., mean squared error (MSE).", "coarse": "arg_structuring", "fine": "arg-structuring_quote", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 10, "text": "But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem min_\u03b2 (1/N)\u03a3_n (y_n \u2013 x_n \u2022\u00a0\u03b2)^2 with the minibatch gradient estimator computed over randomly chosen minibatches B: \\hat g = (1/|B|) \u03a3_{n \\in B} x_n (y_n \u2013 x_n \u2022\u00a0\u03b2).", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 14, "text": "As the norm of \u03b2 goes to infinity, so does the expected norm of the error of \\hat g. I'm not saying this is a particularly big deal, just that it's not an improvement over Yan et al.'s result.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 16, "text": "That aside, this seems like good work that could have a significant impact on practice.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_motivation-impact", "pol": "pol_positive"}, {"review_id": "S1gTSur19S", "sentence_index": 17, "text": "A couple of other minor points:", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 18, "text": "\u2022", "coarse": "arg_structuring", "fine": "arg-structuring_heading", "asp": "none", "pol": "none"}, {"review_id": "S1gTSur19S", "sentence_index": 19, "text": "It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 20, "text": "It would be nice to see some discussion (or at least speculation) on why that is.", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_substance", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 21, "text": "\u2022\u00a0Not all of the arrows in Figure 1 are pointing to the right lines.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "S1gTSur19S", "sentence_index": 22, "text": "\u2022\u00a0In the abstract, it might be good to clarify that the exponentiation is elementwise.", "coarse": "arg_request", "fine": "arg-request_edit", "asp": "asp_clarity", "pol": "pol_negative"}], "rebuttal_sentences": [{"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 0, "text": "We thank the reviewer for the positive assessment of our work.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_global", null]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 1, "text": "We would like start by stating that we did not mean to claim that the rate of convergence proved in this paper is better that than of Yan et al.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 2, "text": "We have modified the Remarks to clarify the statements.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 3, "text": "In the stochastic gradient setting, the number of gradient evaluation is indeed $T^2$. This is consistent with the result in Bernstein et al. (2018).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 4, "text": "The main point we would like to make is that the bounds are very concise and exactly reduce to that of gradient descent/stochastic gradient descent in the special cases.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 5, "text": "We thank you for pointing out that the bounded variance assumption may also be restrictive and only satisfied on bounded domains.", "coarse": "nonarg", "fine": "rebuttal_social", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 6, "text": "It is nonetheless a standard assumption made in the literature.", "coarse": "dispute", "fine": "rebuttal_mitigate-criticism", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 7, "text": "We have modified Remark 3.4 (and added Remark 3.5) to make this clear in the updated version.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 8, "text": "Response to other minor points:", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [17, 19, 20, 21, 22]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 9, "text": "Our convergence analysis is done for non-convex objective functions (similar to that of Yan et al. and Bernstein et al.).", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 10, "text": "In the non-convex setting, to the best of our knowledge, there are no theoretical results that show benefits of momentum methods over SGD.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 11, "text": "For experiments, we speculate that the reason is that the batch size used is too small for (Powered)SGDM to gain an advantage over (Powered)SGD.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 12, "text": "We plan to add SGD as a reference algorithm (as suggested by another reviewer).", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 13, "text": "Once the experiments are complete, we should be able to see how SGDM compares with SGD in the experiments.", "coarse": "nonarg", "fine": "rebuttal_summary", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 14, "text": "This may take a while for the ImageNet experiments, but we promise to do so in the final version.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [19, 20]]}, {"review_id": "S1gTSur19S", "rebuttal_id": "BJgxTZgSir", "sentence_index": 15, "text": "We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know.", "coarse": "concur", "fine": "rebuttal_by-cr", "alignment": ["context_sentences", [21, 22]]}]}