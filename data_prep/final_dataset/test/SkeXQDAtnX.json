{"metadata": {"forum_id": "ByeWdiR5Ym", "review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "title": "Adaptive Convolutional Neural Networks", "reviewer": "AnonReviewer1", "rating": 4, "conference": "ICLR2019", "permalink": "https://openreview.net/forum?id=ByeWdiR5Ym&noteId=SylUZyX7AX", "annotator": "anno10"}, "review_sentences": [{"review_id": "SkeXQDAtnX", "sentence_index": 0, "text": "The paper develops a new 'convolution' operation.", "coarse": "arg_structuring", "fine": "arg-structuring_summary", "asp": "none", "pol": "none"}, {"review_id": "SkeXQDAtnX", "sentence_index": 1, "text": "I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkeXQDAtnX", "sentence_index": 2, "text": "p2-3, Section 3.1 - I found the equations impossible to read. What are the subscripts over?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_clarity", "pol": "pol_negative"}, {"review_id": "SkeXQDAtnX", "sentence_index": 4, "text": "In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkeXQDAtnX", "sentence_index": 5, "text": "Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_soundness-correctness", "pol": "pol_negative"}, {"review_id": "SkeXQDAtnX", "sentence_index": 6, "text": "Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?", "coarse": "arg_request", "fine": "arg-request_explanation", "asp": "asp_replicability", "pol": "pol_negative"}, {"review_id": "SkeXQDAtnX", "sentence_index": 7, "text": "Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost.", "coarse": "arg_evaluative", "fine": "none", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkeXQDAtnX", "sentence_index": 8, "text": "It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_positive"}, {"review_id": "SkeXQDAtnX", "sentence_index": 9, "text": "It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.", "coarse": "arg_request", "fine": "arg-request_experiment", "asp": "asp_substance", "pol": "pol_neutral"}], "rebuttal_sentences": [{"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 0, "text": "I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [1]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 1, "text": "R)We believe as future work our algorithm can be combined with Winograd techniques for optimization.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 2, "text": "For instance winograd is designed to use a batch of images to convolve with a kernel, here an image convolves with a \u201cbatch of kernels\u201d.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 3, "text": "There is no reason why those two techniques can be merged.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 4, "text": "In our implementation we perform a set of convolutions with the input image where FFT can be applied too.", "coarse": "dispute", "fine": "rebuttal_reject-criticism", "alignment": ["context_sentences", [1]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 5, "text": "p2-3, Section 3.1 - I found the equations impossible to read. What", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 6, "text": "are the subscripts over?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [2]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 7, "text": "In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [4]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 8, "text": "Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [5]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 9, "text": "Very good Catch", "coarse": "concur", "fine": "rebuttal_concede-criticism", "alignment": ["context_sentences", [2, 2, 4, 5]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 10, "text": "it should be (N)x(N) instead of (N+1)x(N+1). (Fixed on the paper)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [2, 2, 4, 5]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 11, "text": "Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [6]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 12, "text": "{k,l} locate the convolving window inside of the input image", "coarse": "concur", "fine": "rebuttal_answer", "alignment": ["context_sentences", [6]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 13, "text": "Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [7]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 14, "text": "It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [8]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 15, "text": "R)We compared now against: mobileNet, ShuffleNet, HENet, SqueezeNet, we have less number of parameters or better accuracy or both.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 16, "text": "For instance our method has 4X les parameters than shuffleNet and better accuracy (Added to the paper)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [7, 8]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 17, "text": "It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.", "coarse": "nonarg", "fine": "rebuttal_structuring", "alignment": ["context_sentences", [9]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 18, "text": "R)In this paper we focus on the reduction of parameters, we didn\u2019t focus on the speed, we notice that in our experiment our models were trained using half of the epoch used for the conventional models.", "coarse": "dispute", "fine": "rebuttal_refute-question", "alignment": ["context_sentences", [9]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 19, "text": "In terms of the number of operations the LeNet as in the tutorial has 2.29M MAC operations, while our method has 1.23M MAC operations for MNIST.", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}, {"review_id": "SkeXQDAtnX", "rebuttal_id": "SylUZyX7AX", "sentence_index": 20, "text": "(Added to the paper)", "coarse": "concur", "fine": "rebuttal_done", "alignment": ["context_sentences", [9]]}]}