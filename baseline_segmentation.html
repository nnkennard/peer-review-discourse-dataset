
<HTML>
   <head>
      <title> Review-rebuttal segmentation viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h2 class="title is-2"> Segmentations for H1xSQUW2tS </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>This paper introduced a latent space model for reinforcement learning in vision-based control tasks.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>It first learns a latent dynamics model, in which the transition model and the reward model can be learned on the latent state representations.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Using the learned latent state representations, it used an actor-critic model to learn a reactive policy to optimize the agent's behaviors in long-horizon continuous control tasks.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The method is applied to vision-based continuous control in 20 tasks in the Deepmind control suite.<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Pros:<br><br></td><td>*</td><td>structuring|heading</td><td>@</td></tr>
<tr><td>1. The method used a latent dynamics model, which avoids reconstruction of the future images during inference.<br><br></td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.<br><br></td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.<br><br><br><br></td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>Cons:<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.</td><td>@</td><td>evaluative</td><td>@</td></tr>
<tr><td>In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.</td><td>@</td><td>structuring|summary</td><td>@</td></tr>
<tr><td>However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).<br><br><br><br></td><td>@</td><td>request|experiment</td><td>@</td></tr>
<tr><td>2. Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.<br><br><br><br></td><td>*</td><td>request|experiment</td><td>*</td></tr>
<tr><td>3. The world model is fixed while learning the action and value models, meaning that reinforcement learning of the actor-critic model cannot be used to improve the latent state model.</td><td>@</td><td>fact</td><td>@</td></tr>
<tr><td>It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.<br><br><br><br></td><td>@</td><td>request|experiment</td><td>@</td></tr>
<tr><td>Typos:<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>Reward prediction along --> Reward prediction alone<br><br></td><td></td><td>request|typo</td><td>@</td></tr>
<tr><td>this limitation in latenby?</td><td></td><td>request|typo</td><td>@</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Thank you for your review!<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>> Pros:<br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>> 1.</td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>The method used a latent dynamics model, which avoids reconstruction of the future images during inference.<br><br></td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>> 2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.<br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>> 3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.<br><br><br><br></td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>This is an accurate summary.</td><td>@</td><td>summary</td><td>@</td></tr>
<tr><td>We would like to highlight two additional points.</td><td></td><td>structuring</td><td>@</td></tr>
<tr><td>First, the improved performance is attributed to a novel actor-critic algorithm that uses analytic multi-step gradients of predicted state-values (not Q-values).</td><td></td><td>other</td><td>@</td></tr>
<tr><td>Second, in addition to outperforming previous latent space planning methods, the proposed algorithm also outperforms the model-free D4PG algorithm, the previous state-of-the-art on this benchmark suite.<br><br><br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>> 1.</td><td>*</td><td>structuring</td><td>@</td></tr>
<tr><td>The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.</td><td>*</td><td>structuring</td><td>@</td></tr>
<tr><td>In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.</td><td>*</td><td>structuring</td><td>@</td></tr>
<tr><td>However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).<br><br><br><br></td><td>*</td><td>structuring</td><td>@</td></tr>
<tr><td>Dreamer is a novel algorithm that belongs to the family of actor critic methods.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE).</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>First, they learn a Q function rather than just a V function.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Instead, they only serve for computing multi-step Q targets for learning the Q critic.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.<br><br><br><br></td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>> 2.</td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.<br><br><br><br></td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>As summarized above, Dreamer differs from previous actor-critic algorithms not just by using latent dynamics but also by using analytic multi-step gradients of a V function rather than one-step gradients Q function.</td><td>@</td><td>refute-question</td><td>@</td></tr>
<tr><td>This renders Dreamer conceptually distinct from DDPG, SAC, MVE, and STEVE.<br><br><br><br></td><td>@</td><td>refute-question</td><td>@</td></tr>
<tr><td>We have run experiments with MVE in the latent space of the same dynamics model and tuned the learning rate for actor and Q function.</td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>We did not find an improvement over Dreamer (MVE worked worse across tasks) in these experiments, possibly because it only updates the Q function at the initial state of the imagination rollout.<br><br><br><br></td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>Note that with a model, Q values can be computed by combining the dynamics with a value function, so learning Q is not necessary anymore.</td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>Since using V in Dreamer outperforms the state-of-the-art D4PG agent and is simpler than the Q function in DDPG and MVE and substantially simpler than STEVE (ensemble of models) and SAC (two Q functions, one V function), we argue for this design choice.<br><br><br><br></td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>> 3. [...] It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.<br><br><br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>We have run these experiments and it prevented learning completely.</td><td>*</td><td>done</td><td>@</td></tr>
<tr><td>Using gradients of the action or value models to shape the dynamics allows them to "cheat".</td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>Specifically, the actions maximize value estimates; using these to update the dynamics results in overly optimistic dynamics.</td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>The values maximize Bellman consistency; using these to update the dynamics can encourage collapse of the latent space.</td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>As a result, we suggest the perspective of viewing the dynamics as a fixed MDP during imagination training.</td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>We will add a discussion of this to the paper.<br><br><br><br></td><td>*</td><td>by-cr</td><td>@</td></tr>
<tr><td>If we addressed your concerns satisfactorily, we would be happy if you would consider updating your score.</td><td></td><td>social</td><td>@</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for rJlsvSSchm </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>This paper presents a gradient estimator for expectation-based objectives, which is called Go-gradient.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>This estimator is unbiased, has low variance and, in contrast to other previous approaches, applies to either continuous and discrete random variables.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>They also extend this estimator to problems where the gradient should be "backpropagated" through a nested combination of random variables and a (non-linear) functions.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Authors present an extensive experimental evaluation of the estimator on different challenging machine learning problems.<br><br><br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The paper addresses a relevant problem which appears in many machine learning settings, as it is the problem of estimating the gradient of an expectation-based objective.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>In general, the paper is well written and easy to follow. And the experimental evaluation is extensive and compares with relevant state-of-the-art methods.<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>The main problem with this paper is that it is difficult to identify its main and novel contributions.<br><br><br><br></td><td>*</td><td>evaluative</td><td>*</td></tr>
<tr><td>1. In the case of continuous random variables, Go-gradient is equal to Implicit Rep gradients (Figurnov et al. 2018) and pathwise gradients (Jankowiack & Obermeyer,2018).</td><td>@</td><td>fact</td><td>*</td></tr>
<tr><td>Furthermore, for the Gaussian case, Implicit Rep gradients (and Go-gradient too) are equal to the standard reparametrization trick estimator (Kingma & Welling, 2014).</td><td>@</td><td>fact</td><td>*</td></tr>
<tr><td>This should be made crystal-clear in the paper.</td><td>@</td><td>request|edit</td><td>*</td></tr>
<tr><td>What happens is that the authors arrive at this solution using a different approach.<br><br><br><br></td><td>@</td><td>fact</td><td>*</td></tr>
<tr><td>In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.</td><td></td><td>request|edit</td><td>@</td></tr>
<tr><td>Moreover, I don't think some of the presented experiments are necessary.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>Simply because for continuous variables similar experiments have been reported before (Figurnov et al. 2018, Jankowiack & Obermeyer,2018).<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>2. It seems that the main novel contribution of the paper is to extend the ideas of (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) to discrete variables. And this is a relevant contribution.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>And the experimental evaluations of this part are convincing and compare favourably with other state-of-the-art methods.<br><br><br><br></td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>3. Authors should be much more clear about which is their original contribution to the problems stated in Section 4 and Section 5. As authors acknowledge in Section 6 .</td><td>*</td><td>request|edit</td><td>*</td></tr>
<tr><td><<Stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015), focusing mainly on re-parameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through several continuous random variables. >></td><td>@</td><td>fact</td><td>*</td></tr>
<tr><td>This is exactly what authors do in these sections.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Again it seems that the real contribution of this paper here is to extend this stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) ideas to discrete variables.</td><td></td><td>fact</td><td>*</td></tr>
<tr><td>Although this extension seems to be easily derived using the contributions made at point 2.<br><br><br><br></td><td>*</td><td>evaluative</td><td>*</td></tr>
<tr><td>Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.</td><td></td><td>evaluative</td><td>@</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Thank you for your time and effort of reviewing our paper. Please see our response below.<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>Our main contributions include:<br><br><br><br></td><td></td><td>structuring</td><td>*</td></tr>
<tr><td>(i) For single-layer random variables (RVs), we propose a unified gradient named GO by exploiting the integration-by-parts idea, which is applicable to continuous/discrete RVs.</td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>In the special case of single-layer continuous RVs where GO recovers Implicit Rep or pathwise gradients, we consider it’s our contribution to provide a principled explanation (via integration-by-parts) why Implicit Rep and pathwise gradients have low Monte Carlo variance; or in other words, we prove that their implicit differentiation originates from integration-by-parts.<br><br><br><br></td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>(ii) For multi-layer RVs, our main contribution is the discovery that with GO (or in other words, the introduced variable-nabla), one can back-propagate gradient information through a nested combination of nonlinear functions and general RVs (including non-reparameterizable continuous RVs, back-propagating through which is challenging).</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>Another interpretation of this contribution is that GO enables generalizing the deterministic chain rule to a statistical version.</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>Here, we refer to deterministic chain rule as back-propagating gradient through deterministic functions (like neural networks) or reparameterizable RVs (like Gaussian).</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>By contrast, statistical chain rule is referred to as back-propagating gradient through more general RVs (including non-reparameterizable ones).</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>Of course, statistical chain rule recovers deterministic chain rule for deterministic functions and reparameterizable RVs, because GO recovers the standard Rep.<br><br><br><br></td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>(iii) Another 2 minor contributions include Lemma 1 and Corollary 1.</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>In Lemma 1, we explicitly prove that our deep GO gradient contains the standard Rep as a special case, in general beyond Gaussian.</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>Note neither Implicit Rep nor pathwise gradients can recover Rep in general, because a neural-network-parameterized reparameterization usually leads to a nontrivial CDF.</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>In Corollary 1, we reveal the fact that the proposed method degrades into the classical back-propagation algorithm under specific settings.<br><br><br><br></td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>Finally, we believe it is interesting to create a consistent architecture, which unifies (a) a GO gradient which contains many popular gradients as special cases, and (b) a more general statistical chain rule developed based on GO which recovers the well-known deterministic chain rule under specific cases.<br><br><br><br></td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>For your comments not addressed above, please see our additional response below.<br><br><br><br></td><td></td><td>structuring</td><td>*</td></tr>
<tr><td>(1) We have made clearer the relationships among the standard Rep, Implicit Rep/pathwise, and our GO in the revised manuscript.</td><td>@</td><td>done</td><td>*</td></tr>
<tr><td>In the revised paper we have explicitly pointed out that the experiments from (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) additionally support our GO in the special case of single-layer continuous RVs.<br><br><br><br></td><td>@</td><td>summary</td><td>*</td></tr>
<tr><td>(2) Please refer to our main contributions summarized above, where other contributions, beyond GO for discrete RVs, are clarified.<br><br><br><br></td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>(3) Please refer to our main contributions (ii)-(iii).</td><td>@</td><td>summary</td><td>@</td></tr>
<tr><td>As stated in our paper, many works tried to solve the problem of stochastic/statistical back-propagation.</td><td>@</td><td>summary</td><td>@</td></tr>
<tr><td>We consider our contributions in Secs. 4 and 5 as one step toward that final goal.</td><td>@</td><td>summary</td><td>@</td></tr>
<tr><td>Please note that what’s done in Secs. 4 and 5 is not straight-forward and has not been reported before.</td><td>@</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>Since stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) focuses mainly on reparameterizable RVs, deterministic chain rule as mentioned in main contribution (ii) can be readily applied.</td><td>@</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>By contrast, we target towards more general situations in Secs. 4 and 5 where deterministic chain rule might not be applicable, such as for non-parameterizable (continuous) RVs.</td><td>@</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>We prove that one can utilize our GO to sequentially back-propagate gradient though non-parameterizable continuous RVs, namely the statistical chain rule mentioned in main contribution (ii).<br><br><br><br></td><td>@</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>We have revised the last paragraph of the Introduction to make a more explicit summation of our main contributions, as mentioned above.<br><br><br><br></td><td></td><td>done</td><td>*</td></tr>
<tr><td>We hope your concerns have been addressed. If not, further discussion would be welcomed.</td><td></td><td>social</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for HkxrqOb6nm </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>The authors propose a new on-policy exploration strategy by using a policy with a hierarchy of stochasticity.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The authors use a two-level hierarchical distribution as a policy, where the global variable is used for dropout.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>This work is interesting since the authors use dropout for policy learning and exploration.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The authors show that parameter noise exploration is a particular case of the proposed policy.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The main concern is the gap between the problem formulation and the actual optimization problem in Eq 12.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>I am very happy to give a higher rating if the authors address the following points.<br><br><br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>Detailed Comments<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>(1) The authors give the derivation for Eq 10.</td><td>*</td><td>fact</td><td>@</td></tr>
<tr><td>However, it is not obvious that how to move from line 3 to line 4 at Eq 15.<br><br></td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>Minor:  Since the action is denoted by "a",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of "\alpha" at Eq 10 and 15.<br><br><br><br></td><td></td><td>request|typo</td><td>@</td></tr>
<tr><td>(2) Due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at Eq 12.</td><td>@</td><td>fact</td><td>*</td></tr>
<tr><td>Does such approximation guarantee the policy improvement?</td><td>@</td><td>request|explanation</td><td>*</td></tr>
<tr><td>Any justification?<br><br><br><br></td><td>@</td><td>request|explanation</td><td>*</td></tr>
<tr><td>(3) Instead of using the mean policy approximation in Eq 12, the authors should consider existing Monte Carlo techniques to reduce the variance of the gradient estimation.</td><td>*</td><td>request|experiment</td><td>*</td></tr>
<tr><td>For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.</td><td>*</td><td>request|experiment</td><td>*</td></tr>
<tr><td>Note that the gradient is biased if the mean policy approximation is used.<br><br><br><br></td><td>*</td><td>fact</td><td>*</td></tr>
<tr><td>(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?</td><td>@</td><td>request|explanation</td><td>@</td></tr>
<tr><td>The authors should clarify this point.<br><br><br><br></td><td>@</td><td>request|explanation</td><td>@</td></tr>
<tr><td>(5) Due to the mean policy approximation, does the mean policy depend on \phi?</td><td>*</td><td>request|explanation</td><td>@</td></tr>
<tr><td>The authors should clearly explain how to update \phi when optimizing Eq 12.<br><br><br><br></td><td>*</td><td>request|explanation</td><td>@</td></tr>
<tr><td>(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?<br><br><br><br></td><td></td><td>request|explanation</td><td>@</td></tr>
<tr><td>(7) The authors give the derivations about \theta such as the gradient and the regularization term about \theta (see, Eq 18-19).</td><td>@</td><td>fact</td><td>*</td></tr>
<tr><td>However, the derivations about \phi are missing.</td><td>@</td><td>evaluative</td><td>*</td></tr>
<tr><td>For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.<br><br></td><td>@</td><td>request|experiment</td><td>*</td></tr>
<tr><td>Minor, 1/2 is missing in the last line of Eq 19.<br><br><br><br></td><td></td><td>request|typo</td><td>*</td></tr>
<tr><td>Reference:<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>[1] AUEB, Michalis Titsias RC, and Miguel Lázaro-Gredilla. "Local expectation gradients for black box variational inference." In Advances in neural information processing systems, pp. 2638-2646. 2015.</td><td></td><td>other</td><td>*</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Thank your very much for your review.</td><td></td><td>social</td><td>*</td></tr>
<tr><td>We have updated the manuscript with more details in the derivation of the first order approximation of KL divergence.<br><br><br><br></td><td></td><td>done</td><td>*</td></tr>
<tr><td>1) Elaborated derivation of Eq. 10<br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>Q1: We have added one more line to explain the derivation.</td><td>*</td><td>done</td><td>*</td></tr>
<tr><td>Basically a baseline is subtracted, and GAE is introduced.<br><br><br><br></td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>2) Gradient update on \phi from KL divergence<br><br></td><td></td><td>structuring</td><td>@</td></tr>
<tr><td>The gradients w.r.t. \phi from the KL divergence is stopped for variance reduction with acceptable bias, which we prove with MuProp [1].</td><td>@</td><td>summary</td><td>@</td></tr>
<tr><td>Details could be found in Appendix C.<br><br></td><td>@</td><td>summary</td><td>@</td></tr>
<tr><td>Q3: Rather than [2], we employ MuProp to reduce variance in our development of NADPEx.</td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>Thank your for your suggestion.<br><br></td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>Q4: Yes \theta and \phi are jointly and simultaneously optimized at Eq. 12, though the gradients w.r.t. \phi from the KL divergence is stopped.<br><br></td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>Q7: Due to the stop-gradient manipulation in the KL divergence, gradients w.r.t. \phi remains the same as in stated in last subsection.<br><br><br><br></td><td>@</td><td>refute-question</td><td>@</td></tr>
<tr><td>3) Mean policy in the KL divergence<br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>What motivates the mean policy is not variance reduction, but the idea that dropout policy had better to be close to each other.</td><td>*</td><td>refute-question</td><td>*</td></tr>
<tr><td>As intuitively \phi is controlling the distance between dropout policies, it would further remedy the little bias mentioned above.</td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>However, the computation complexity for "close to each other" would be O(N^2), with N being the number of dropout policies in this batch.</td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>We employ mean policy to make it linear. And it could be regarded as an integration on a Gaussian approximation of the Monte Carlo estimate according to [3].</td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>Details could be found in Appendix C.<br><br></td><td>*</td><td>summary</td><td>*</td></tr>
<tr><td>Q2: No the mean policy is not used due to the likelihood ratio trick.</td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>And the approximation of using mean policy is discussed in [3], with a sound deduction.<br><br></td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>Q3: Mean policy is not motivated by variance reduction, which is addressed as introduced above.</td><td>*</td><td>refute-question</td><td>*</td></tr>
<tr><td>Thank you for your suggestion.<br><br></td><td>*</td><td>social</td><td>*</td></tr>
<tr><td>Q5: In the updated version, we have explicitly pointed out that the gradients w.r.t. \phi from KL divergence is stopped. Thanks for this suggestion.<br><br><br><br></td><td>@</td><td>done</td><td>*</td></tr>
<tr><td>Hope our response addresses your concerns!<br><br><br><br></td><td></td><td>social</td><td>@</td></tr>
<tr><td>[1] Gu et al., "MuProp: Unbiased Backpropagation for Stochastic Neural Networks", ICLR 2016.<br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>[2] Titsias et al., "Local Expectation Gradients for Black Box Variational Inference", NIPS 2015.<br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>[3] Wang et al., "Fast dropout training", ICML 2013.</td><td></td><td>other</td><td>@</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for Hyxp9P_RuH </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>###Summary###<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>This paper tackles the multi-source domain adaptation by aggregate multiple source domains dynamically during the training phase.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The observation is that in many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Firstly, the paper derives a multiple-source domain adaptation upper-bound from single-to-single domain adaptation generalization bound, based on the theoretical work from Cortes et al (2019).</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The idea is similar to Zhao et al (2019), which introduces a weighted parameter \alpha to combine the source domains together.<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Secondly, based on the theoretical result, the paper proposes an algorithm to minimize the upper bound of the theoretical result.</td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>The upper bound can be simplified as the quartic form (Eq. 4) and can be optimized with the Lagrangian form.</td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>Since no closed-form expression for the optimal v can be derived, the authors propose to use binary search to find it.<br><br><br><br></td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>Based on the theoretical results and the algorithm, the paper introduces Domain AggRegation Network (DARN), which contains a base network for feature extraction, h_y to minimize the task loss and h_d to evaluate the discrepancy between each source domain and target domain.</td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>The loss is aggregation with the parameter \alpha.<br><br><br><br></td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>Finally, the paper conduct experiments on sentimental analysis benchmark, Amazon Review and digit datasets.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The paper selects MDAN, DANN, MDMN as the baselines.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>On the amazon review dataset, the performance of the proposed DARN model is comparable with the MDMN baseline.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>On the digit dataset, the model can outperform the baselines.<br><br><br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>### Novelty ## #<br><br><br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>The theoretical results in this paper are extended from Cortes et al (2019) and Zhao et al (2018).</td><td></td><td>fact</td><td>*</td></tr>
<tr><td>Thus, the theoretical contribution of this paper is limited.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>The algorithm proposed in this paper is interesting.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>However, the motivation of the proposed method is to minimize the upper bound, not the loss itself, i.e. L_T(h, f_T).</td><td></td><td>fact</td><td>@</td></tr>
<tr><td>Intuitively, when the upper bound of the loss is minimized, it will be beneficial to minimize the loss itself.</td><td></td><td>fact</td><td>@</td></tr>
<tr><td>But it's not guaranteed as the upper bound contains other variables, such as the number of training samples and model complexity.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>If the training samples and model complexity (think about the parameters in the deep models) are significantly large, the upper bound of the loss might be also very large.<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>The selected baselines are not sufficient.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>The improvement from the baselines is also limited.<br><br><br><br><br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>###Clarity## #<br><br><br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>Overall, the paper is well organized and logically clear.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>The images are well-presented and well-explained by the captions and the text.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>The derivation of the algorithm in Sec 3.2 is logically clear and easy to follow.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>###Pros# ##<br><br><br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>1) The paper proposes a new theoretical upper-bound based on the prior works, the upper-bound and its derivation are interesting and heuristic to the domain adaptation research community.<br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>2) The paper is applicable to many practical scenarios since the data from the real-world application is typically collected from multiple sources.<br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>3) The paper is overall well-organized and well-written.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>The claims of the paper are verified by the experimental results.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>## #Cons# ##<br><br><br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>The idea is intuitive when the upper bound is small.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.<br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>It's an intuitive idea to weight different source domains in multi-source domain adaptation.</td><td></td><td>fact</td><td>@</td></tr>
<tr><td>The paper derives the weight by the Lagrangian form to minimize the upper bound.</td><td></td><td>fact</td><td>@</td></tr>
<tr><td>While another trivial trick is to evaluate \alpha by the domain closeness between each source domain with the target domain.<br><br></td><td></td><td>fact</td><td>@</td></tr>
<tr><td>2) The experimental results provided in this paper are weak.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>In the abstract and introduction ,  the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.</td><td></td><td>fact</td><td>@</td></tr>
<tr><td>But the paper only provides empirical results on sentimental analysis and digit recognition.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>Besides, the results on the sentimental analysis are comparable with the compared baselines.<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:<br><br></td><td></td><td>request|experiment</td><td>*</td></tr>
<tr><td>DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019.</td><td></td><td>other</td><td>*</td></tr>
<tr><td>http://ai.bu.edu/DomainNet/<br><br></td><td></td><td>other</td><td>*</td></tr>
<tr><td>Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017.</td><td></td><td>other</td><td>*</td></tr>
<tr><td>http://hemanthdv.org/OfficeHome-Dataset/<br><br><br><br></td><td></td><td>other</td><td>*</td></tr>
<tr><td>3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.<br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>To improve the rating, the author should explain the following questions:<br><br></td><td></td><td>request|explanation</td><td>@</td></tr>
<tr><td>1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?<br><br></td><td></td><td>request|explanation</td><td>@</td></tr>
<tr><td>2) .</td><td></td><td>other</td><td>@</td></tr>
<tr><td>In the introduction, the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.</td><td></td><td>fact</td><td>@</td></tr>
<tr><td>While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?</td><td></td><td>request|experiment</td><td>@</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Thank you for your thorough assessment and helpful comments! To answer your two questions:<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>1) Upper bound<br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>You are right that it would be ideal to optimize the target loss L_T(h, f_T) directly.</td><td>*</td><td>reject-request</td><td>*</td></tr>
<tr><td>However, this is not possible because we do not have labelled target data (i.e., f_T is unknown).</td><td>*</td><td>reject-request</td><td>*</td></tr>
<tr><td>Minimizing an upper bound is arguably the only viable option *with theoretical generalization guarantee*. It is a common practice in the domain adaptation community (Mansour et al., 2009a, 2009b; Ben-David et al. 2007, 2010; Cortes and Mohri, 2011), and it is essentially the key idea of PAC learning and generalization analysis (Györfi et al., 2006; Schölkopf et al., 2002; Vapnik, 2013).</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Besides, our method *directly* optimizes the upper bound without resorting to heuristics, unlike prior methods.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Ref:<br><br></td><td></td><td>structuring</td><td>@</td></tr>
<tr><td>- Györfi, L., Kohler, M., Krzyzak, A. and Walk, H., 2006. A distribution-free theory of nonparametric regression. Springer Science & Business Media.<br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>- Schölkopf, B., Smola, A.J. and Bach, F., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.<br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>- Vapnik, V., 2013. The nature of statistical learning theory. Springer science & business media.<br><br><br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>2) More experiment<br><br></td><td></td><td>structuring</td><td>@</td></tr>
<tr><td>Thank you for pointing out these datasets.</td><td>@</td><td>concede-criticism</td><td>@</td></tr>
<tr><td>We have conducted further experiments on the Office-Home dataset,  using the ResNet50 as the backbone architecture and changing the classification head to 65 classes.</td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>As we can see in the new Section 5.3, our method achieve state-of-the-art performance and outperform all alternatives</td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>; these results are statistically significant.<br><br><br><br></td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>In addition, we have added one more competitive baseline (M3SDA) from the DomainNet paper you mentioned, using their public code with a few necessary adjustments for each dataset (e.g., network architecture, etc).</td><td>@</td><td>done</td><td>*</td></tr>
<tr><td>We ensure that all methods use the same backbone architecture for a fair comparison.</td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>Again, our method outperform M3SDA in all datasets.<br><br><br><br></td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>If you have any other comments or concerns, we are happy to provide further feedback.</td><td></td><td>social</td><td>*</td></tr>
<tr><td>Thank you!</td><td></td><td>social</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for SyeS7CQ83m </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Summary:<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>This paper proposes a generative point cloud model based on adversarial learning and definitti’s representation theorem of exchangeable variables.<br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify).<br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.<br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The model consists of thee components: (i) an “encoder” that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here (ii) a “decoder” that takes the estimated global latent variable, and a local latent variable, and maps it to an “output” point in the cloud to be produced by the model. (iii) a “discriminator” network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder. Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape. (iv) a “shape prior” that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>As compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number.<br><br><br><br></td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>In addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes. This approach translates to improved quantitative evaluation measures, Experiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40.<br><br></td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>Two performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated.<br><br><br><br></td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>Overall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work.<br><br></td><td></td><td>fact</td><td>*</td></tr>
<tr><td>The experimental validation of the proposed approach can also be further improved, see more specific comments below.<br><br><br><br></td><td></td><td>fact</td><td>*</td></tr>
<tr><td>Specific comments:<br><br><br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>- The notation in section 3 (before 3.1) is rather sloppy. For example, - please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3. - it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \sim p(theta).<br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>- what prior distributions p(z) and p(u) are used? What is the choice based on?<br><br><br><br></td><td></td><td>request|explanation</td><td>@</td></tr>
<tr><td>- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier.<br><br><br><br></td><td></td><td>request|explanation</td><td>*</td></tr>
<tr><td>- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?<br><br><br><br></td><td></td><td>request|clarification</td><td>*</td></tr>
<tr><td>- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.<br><br><br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>- The following paper merits a discussion in the related work section:<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>“TOWARDS A NEURAL STATISTICIAN”, ICLR’17,</td><td>*</td><td>other</td><td>@</td></tr>
<tr><td>https://openreview.net/pdf?id=HJDBUF5le<br><br><br><br></td><td></td><td>other</td><td>@</td></tr>
<tr><td>- The manuscript contains many typos. For example<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>“vedio” op page 4, “circile” on page 5, “condct” on page 8, etc.<br><br></td><td></td><td>request|typo</td><td>*</td></tr>
<tr><td>Please proof read your paper and fix these.<br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>The refenence to  Bengio 2018 is incomplete: what do you refer to precisely?<br><br><br><br></td><td></td><td>request|clarification</td><td>*</td></tr>
<tr><td>- There seems to be no mention of the dimension of the “local” latent variables z_i.<br><br></td><td></td><td>fact</td><td>*</td></tr>
<tr><td>Please comment on the choice, and its impact on the behavior of the model.<br><br><br><br></td><td></td><td>request|explanation</td><td>*</td></tr>
<tr><td>- The quantitative evaluation in table 1 is interesting and useful.<br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.<br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Quantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature. Could you please comment on how this can/will be fixed?<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>- The toy data set experiments could be dropped  to make room for experiments suggested below.<br><br><br><br></td><td></td><td>request|edit</td><td>*</td></tr>
<tr><td>- An experimental study of the effect of the mixing parameter “s” would be useful to include. For example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.<br><br><br><br></td><td></td><td>request|experiment</td><td>*</td></tr>
<tr><td>- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?<br><br><br><br></td><td></td><td>request|experiment</td><td>*</td></tr>
<tr><td>- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we “chop off” part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts? This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners.<br><br><br><br></td><td></td><td>request|experiment</td><td>@</td></tr>
<tr><td>- Analysis of shapes with different genus and dimensions would be interesting.<br><br></td><td></td><td>request|result</td><td>@</td></tr>
<tr><td>Does the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk), despite a simple prior on the local latent variables z?</td><td></td><td>request|explanation</td><td>@</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>1. We apologize for typos and if any term is not defined at the appropriate places.</td><td></td><td>structuring</td><td>*</td></tr>
<tr><td>We  fixed all the typos and define the abbreviation for IPM at the first occurrence.</td><td></td><td>done</td><td>*</td></tr>
<tr><td>Please check the revision.</td><td></td><td>done</td><td>*</td></tr>
<tr><td>P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3, is defined in the subsequent two sentences in the same paragraph.</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>By the notation G_theta(u) \sim p(theta), we mean that we want to train the generator G_theta such that when fed a random variable u \sim p(u), the distribution of G_theta(u) matches that of p(theta).</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>Sorry if it is confusing, but G_theta is not parameterized by theta, it just indicates that its the generator for theta. (Like G_x indicates that it is the generator for x).<br><br><br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>2. The training of G_theta is described in the subsection titled “Hierarchical Sampling”.</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>As correctly pointed out by the reviewer, that G_theta does not appear in the objective function (4).</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>Using (4), we train G_x and Q networks.</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>After training G_x and Q, we use trained Q to collect inferred Q(X), for each point cloud X.</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>Then we train the generator G_theta using ordinary WGAN formulation to produce samples from same distribution as that of the samples Q(X) for each point cloud X. In addition to such two step training, a joint training also works, but is slower computationally, thus we report only the two step training in the paper.<br><br><br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>3. Quantitative evaluation of generative modeling performance is unfortunately very hard for real world problems like point clouds, which is the probable cause for it being missing from much of GAN literature.</td><td>@</td><td>answer</td><td>@</td></tr>
<tr><td>Thus, to provide some quantitative results for generation, we resorted to the toy problem.</td><td>@</td><td>answer</td><td>@</td></tr>
<tr><td>In the toy problem, we can accurately gauge the generation capabilities as can be seen from Figure 5.</td><td>@</td><td>answer</td><td>@</td></tr>
<tr><td>(We did not explicitly provide numbers like KL divergence, as it is evident from the Figure that PC-GAN would be significantly better than AAEs if we evaluate the numbers.) The same protocol can be extended for measuring the quality of the final hierarchical sampling.<br><br><br><br></td><td>@</td><td>answer</td><td>@</td></tr>
<tr><td>4. To showcase the effect of varying s, we chose the reasonable sized ModelNet10 dataset and ran for s=0, s=1, and three values s_1<s_2<s_3 in between.</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>The results are as follows:<br><br><br><br><br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>D2F (Distance to Face)       Coverage<br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>s=0                        6.03E+00                     3.36E-01<br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>s1</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>6.06E+00                     3.41E-01<br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>s2</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>5.77E+00                     3.47E-01<br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>s3</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>6.85E+00                     3.56E-01<br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>s=1</td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>9.19E+00                     3.67E-01<br><br><br><br></td><td>*</td><td>answer</td><td>*</td></tr>
<tr><td>4. Yes the model nicely captures simple topological features of the object, like presence of holes versus being one solid object. Even in the latent space, objects with hole group together.</td><td>@</td><td>answer</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for BJgmhEfTcH </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>This paper is an empirical contribution regarding SGD arguing that it presents two different behaviors which the authors name a noise dominated regimen, and a curvature dominated regime.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>They observe that the behaviors seem to arise in different batch sizes<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The authors derive empirical conclusions and perform experiments in different settings.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The paper is well-written and the experimental setup seems to be carefully carried out.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.</td><td>*</td><td>evaluative</td><td>*</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>We thank the reviewer for their comments.<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>Although our primary contributions are empirical, we also provided a detailed theoretical discussion in section 2, where we give a clear and simple account of why the two regimes arise.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>We would also like to emphasize that we make a significant contribution to the debate regarding SGD and generalization.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>While many papers have proposed that small batches may generalize better than large minibatches, it was recently pointed out by Shallue et al. that none of these experiments provide convincing evidence for this claim, because no experiment to date has compared small and large batch training under a constant step budget with a realistic learning rate decay schedule while independently tuning the learning rate at each batch size.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>We are the first to run this experiment and conclusively establish that SGD noise does enhance generalization in popular models/datasets.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>We believe this is an important contribution.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>We also provide intriguing results as we vary the epoch budget, which demonstrate that the optimal learning rate which maximizes the test accuracy does not decrease as the epoch budget rises.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>This supports the notion that SGD has an optimal “temperature” which biases it towards solutions that generalize well.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Additional experiments in the appendix G go further and study how the optimal learning rate schedule changes as we increase the epoch budget.</td><td>*</td><td>reject-criticism</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for HylcynXA2X </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>PROS:<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>* Original idea of using separate "discriminator" paths for unknown classes<br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>* Thorough theoretical explanation *</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>A variety of experiments<br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>* Very well-written, and clear paper<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>CONS:<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>* The biggest problem for me was the unconvincing results. MNIST-to-MNIST-M has better baselines  (PixelDA performed better on this task for example), Office is not suitable for domain adaptation experiments anymore unless one wants to be in a few-datasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for NN-based domain adaptation); the results on CELL were not convincing, I don't know the dataset but it seems that baseline NN does better than DA most of the times.<br><br></td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>* Comparison with other methods did not take into account a variety of hyperparameters.</td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one.</td><td></td><td>request|edit</td><td>@</td></tr>
<tr><td>What if the proposed method performs that much better than baselines but they hyperparameters are not set correctly?</td><td>@</td><td>request|explanation</td><td>@</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>We thank the reviewer for their encouraging words.<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>"There are better baselines for MNIST to MNIST-M": yes, the presented method might not outperform all other methods on all other datasets. Still, we hope to convince the reviewer that the results on the other datasets are worth being considered.<br><br><br><br></td><td>*</td><td>mitigate-criticism</td><td>*</td></tr>
<tr><td>"Office is not suitable unless one wants to be in a few-datasample regime or work with data with noisy labels": we would like to point that this regime is quite realistic in Bioimage informatics (noisy, with few samples per class).<br><br><br><br></td><td>*</td><td>mitigate-criticism</td><td>@</td></tr>
<tr><td>"The results on Cell are not convincing": as our goal is multi-domain learning on this dataset, the relevant performance indicator is the average risk over all domains.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>Table 2 details what happens in various categories of cases (on classes with/without labelled samples).</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>Despite the (well-known) degradation of the results on labeled classes when one also considers unlabelled classes, the bottom line is that -- regarding the average risk -- our method outperforms the baseline.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>Importantly, MuLANN results on Cell are significantly better than the baseline on all rows which involve unlabeled classes (rows  3, 6, 9, 13), while remaining not significantly different to the baseline on 6/9 of the other rows.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>"Comparison with other methods did not take into account a variety of hyperparameters".</td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>The reviewer is right.</td><td>@</td><td>concede-criticism</td><td>*</td></tr>
<tr><td>Complementary experiments have thus been performed, and tables 1, 2 updated.</td><td>@</td><td>done</td><td>*</td></tr>
<tr><td>We investigated the impact of varying the learning rate, the weight lambda on the discriminator loss, the weight dzeta of the known-unknown discrimination loss, the learning rate schedule, lambda schedule as well as using different learning rates for pre-trained layers versus from scratch layers (see Table 5 for more detailed information).</td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>These results show a moderate sensitivity of MuLANN, MADA and DANN wrt hyper-parameters and confirm that MuLANN outperforms both MADA and DANN (detailed results available here https://drive.google.com/file/d/1NjtMKF53qmnx4_Jyvh-ofxb0WjzcDvow/view?usp=sharing).</td><td>@</td><td>answer</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for rJgwCU_82Q </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>In active learning, there is generally a trade-off between data efficiency and computational cost.</td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both.</td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>The improvements over basic ensembling are rather minimal, at the cost of extra computation.</td><td>*</td><td>structuring|summary</td><td>@</td></tr>
<tr><td>More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so.</td><td>*</td><td>structuring|summary</td><td>@</td></tr>
<tr><td>On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments).</td><td>*</td><td>structuring|summary</td><td>@</td></tr>
<tr><td>As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout?</td><td>@</td><td>request|explanation</td><td>@</td></tr>
<tr><td>At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?<br><br><br><br></td><td></td><td>request|explanation</td><td>@</td></tr>
<tr><td>The novelty of this method is minimal.</td><td>*</td><td>evaluative</td><td>*</td></tr>
<tr><td>The technique basically fills out the fourth entry in a Punnett square.<br><br><br><br></td><td></td><td>fact</td><td>*</td></tr>
<tr><td>The paper is well-written, has good experiments, and has a comprehensive related work section.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Overall, this paper is good, but is not novel or important enough for acceptance.</td><td>@</td><td>evaluative</td><td>*</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>We thank our third reviewer for his comment.<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>We do understand your concern about the significant increase in computational time.</td><td>*</td><td>concede-criticism</td><td>*</td></tr>
<tr><td>However, we believe that in the context of active learning, the main problem is not related to computational power, rather to the scarcity of data.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Therefore, a better way of making the most out of little data is critical.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>For example, a 10 \% increase for only 300 samples acquired, could make a huge difference in a critical field where active learning is most valuable.</td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>We believe that this is exactly what we manage to achieve with our method and this comes as a result of a better representation of uncertainty during AL.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Furthermore,  Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.</td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>Therefore we use 3 stochastic ensembles for our method.<br><br><br><br></td><td>@</td><td>answer</td><td>*</td></tr>
<tr><td>As for the novelty of this method, although it seems more like an engineering solution, we believe that it makes a significant contribution in the field of deep active learning.</td><td>*</td><td>reject-criticism</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for S1e1sc_GqB </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Experiments on few shot learning show that meta dropout achieves better performance.<br><br><br><br></td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Overally, I think this paper is well motivated and experiments on few shot learning are impressive.</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>I have only two major concerns.<br><br><br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>1. Sec 3.2. According to my understanding, Meta dropout introduces a learnable prior for latent $z$, but the training objective does not require posterior inference and thus no variational inference is needed.</td><td>*</td><td>fact</td><td>@</td></tr>
<tr><td>I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\theta,\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework.<br><br><br><br></td><td>*</td><td>request|edit</td><td>@</td></tr>
<tr><td>2.</td><td></td><td>other</td><td>*</td></tr>
<tr><td>Experiments on adversarial robustness can be further improved.</td><td>@</td><td>request|experiment</td><td>*</td></tr>
<tr><td>(1) the settings and the analysis of adversarial robustness experiment can be discussed in details.</td><td>@</td><td>request|edit</td><td>*</td></tr>
<tr><td>For example, how to build ''adversarial learning baseline'' in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them?</td><td>@</td><td>request|explanation</td><td>*</td></tr>
<tr><td>(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.</td><td>@</td><td>evaluative</td><td>*</td></tr>
<tr><td>I suggest trying some other STOA attack methods (e.g., iterative methods).<br><br><br><br></td><td>@</td><td>request|experiment</td><td>*</td></tr>
<tr><td>Some typos:<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>Page 3, Regularization methods, 3rd line, ````wwwdiscuss<br><br></td><td></td><td>request|typo</td><td>*</td></tr>
<tr><td>Page 7, 2nd line from the bottom, FSGM->FGSM</td><td></td><td>request|typo</td><td>*</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>We really appreciate your constructive comments.</td><td></td><td>social</td><td>*</td></tr>
<tr><td>We respond to each comment as follows.<br><br><br><br></td><td></td><td>structuring</td><td>*</td></tr>
<tr><td>1. Meta dropout does not regularize the variational framework because there is no variational inference framework.<br><br><br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>- Thank you for your comment.</td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>We agree with you that the current lower bound is not a variational form due to the assumption of q=p. In Section 3.2, we toned down the original expression “Learning to regularize variational inference“ into “Connection to variational inference”, and corrected the corresponding sentences.</td><td>*</td><td>concede-criticism</td><td>*</td></tr>
<tr><td>Still, there exists a clear connection between standard variational inference and our learning framework.</td><td>*</td><td>concede-criticism</td><td>*</td></tr>
<tr><td>Thus we believe that discussion in Section 3.2 will be helpful to readers who want to understand the meaning of learning objective Eq.(2) in depth.<br><br><br><br></td><td>*</td><td>concede-criticism</td><td>*</td></tr>
<tr><td>2. Improving adversarial robustness experiment.<br><br><br><br></td><td>@</td><td>structuring</td><td>@</td></tr>
<tr><td>- Thank you for the helpful suggestion.</td><td>@</td><td>social</td><td>@</td></tr>
<tr><td>During the rebuttal period, we conducted additional experiments on adversarial robustness as you suggested:<br><br><br><br></td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>a) We replaced the previous FGSM attack with stronger PGD attack (200 iter.), with $L_1$, $L_2$, and $L_\infty$ norm constraints.<br><br><br><br></td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>b) We included more baselines (e.g. Mixup, VIB, and information dropout), and show that our meta-dropout largely and consistently outperforms all of them.<br><br><br><br></td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>c) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results.<br><br><br><br></td><td>@</td><td>done</td><td>@</td></tr>
<tr><td>d) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $L_1$, $L_2$, and $L_\infty$ attacks.</td><td>@</td><td>done</td><td>*</td></tr>
<tr><td>The generalization to different types of attacks is an important problem in adversarial learning, and most existing models fail to achieve this goal.<br><br><br><br></td><td>@</td><td>done</td><td>*</td></tr>
<tr><td>Please see the corresponding section in the revision.</td><td>@</td><td>social</td><td>*</td></tr>
<tr><td>We believe that the adversarial robustness part of our paper has become much stronger than before, thanks to your suggestion.</td><td>@</td><td>summary</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

 <h2 class="title is-2"> Segmentations for SJeRQb-oFH </h2>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <h3 class="title is-3"> Review </h3><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Paper summary.<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>The paper proposes Dreamer, a model-based RL method for high-dimensional inputs such as images.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>The main novelty in Dreamer is to learn a policy function from latent representation-and-transition models in an end-to-end manner.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Specifically, Dreamer is an actor-critic method that learns an optimal policy by backpropagating re-parameterized gradients through a value function, a latent transition model, and a latent representation model.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>This is unlike existing methods which use model-free or planning methods on simulated trajectories to learn the optimal policy.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Meanwhile, Dreamer learns the remaining components, namely a value function, a latent transition model, and a latent representation model, based on existing methods (the world models and PlaNet).</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Experiments on a large set of continuous control tasks show that Dreamer outperforms existing model-based and model-free methods.<br><br><br><br></td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Comments.<br><br></td><td></td><td>structuring|heading</td><td>*</td></tr>
<tr><td>Efficiently learning a policy from visual inputs is an important research direction in RL.</td><td></td><td>fact</td><td>*</td></tr>
<tr><td>This paper takes a step in this direction by improving existing model-based methods (the world models and PlaNet) using the actor-critic approach.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>I am leaning towards weak accepting the paper.<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>I am reluctant to give a higher score due to its incremental contribution.</td><td>*</td><td>evaluative</td><td>*</td></tr>
<tr><td>Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.</td><td>*</td><td>evaluative</td><td>*</td></tr>
<tr><td>The main difference between Dreamer and SVG is that Dreamer incorporates a latent representation model.</td><td></td><td>structuring|summary</td><td>*</td></tr>
<tr><td>From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.<br><br><br><br></td><td>@</td><td>evaluative</td><td>*</td></tr>
<tr><td>Besides the above comments, I have these additional comments.<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>- Effectiveness on very long horizon trajectories:<br><br></td><td>*</td><td>structuring|heading</td><td>@</td></tr>
<tr><td>Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.</td><td>*</td><td>fact</td><td>@</td></tr>
<tr><td>This is an open issue in model-based RL.</td><td>*</td><td>fact</td><td>@</td></tr>
<tr><td>The paper attempts to solve this issue by backpropagating policy gradients through the transition model, which is known to be more robust against model errors (see e.g., PILCO (Deisenroth et al., 2011)).</td><td></td><td>structuring|summary</td><td>@</td></tr>
<tr><td>However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).</td><td>@</td><td>evaluative</td><td>@</td></tr>
<tr><td>This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).</td><td>@</td><td>fact</td><td>@</td></tr>
<tr><td>I think this point should be discussed in the paper.</td><td>@</td><td>request|edit</td><td>@</td></tr>
<tr><td>That is, the issue still exists, and Dreamer is less effective with very long horizon.<br><br><br><br></td><td>@</td><td>evaluative</td><td>@</td></tr>
<tr><td>- Inapplicability to discrete controls:<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.</td><td>*</td><td>fact</td><td>@</td></tr>
<tr><td>This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.</td><td>*</td><td>evaluative</td><td>@</td></tr>
<tr><td>Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.</td><td>*</td><td>fact</td><td>@</td></tr>
<tr><td>This restriction should be noted in the paper.<br><br><br><br></td><td>*</td><td>request|edit</td><td>@</td></tr>
<tr><td>- There is no mention about variance of policy gradient estimates.</td><td>@</td><td>structuring|summary</td><td>*</td></tr>
<tr><td>Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.<br><br><br><br></td><td>@</td><td>evaluative</td><td>*</td></tr>
<tr><td>- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).</td><td></td><td>evaluative</td><td>*</td></tr>
<tr><td>Also, I suggest moving Section 4 to be right after Section 2, since Section 4 presents existing techniques similarly to Section 2, while Section 3 presents the main contribution.<br><br><br><br><br><br></td><td></td><td>request|edit</td><td>*</td></tr>
<tr><td>Update after authors' response.<br><br></td><td></td><td>structuring|heading</td><td>@</td></tr>
<tr><td>I read the response.</td><td></td><td>social</td><td>@</td></tr>
<tr><td>The paper is more clear after authors' clarification.</td><td></td><td>social</td><td>@</td></tr>
<tr><td>Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).</td><td></td><td>evaluative</td><td>@</td></tr>
<tr><td>Nonetheless, I am keen to acceptance. I would increase the rating from 6 to 7, but I will keep the rating of 6 since the rating of 7 is not possible.</td><td></td><td>social</td><td>@</td></tr></table><h3 class="title is-3"> Rebuttal </h3>Rebuttal <br><table class="table"><thead> <td> Sentence </td><td>alignment</td> <td>label</td> <td>text_tiling</td></thead>
<tr><td>Thank you for the review and accurate summary of our submission!<br><br><br><br></td><td></td><td>social</td><td>*</td></tr>
<tr><td>> I am reluctant to give a higher score due to its incremental contribution.</td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.<br><br><br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>SVG clearly differs from Dreamer in that it only considers 1-step model predictions in SVG(1) or multi-step predictions without value function in SVG(∞).</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>SVG(0) does not use a dynamics model.</td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>In addition, Dreamer propagates gradients through transitions in a learned features, making it effective for high-dimensional control tasks.<br><br><br><br></td><td>*</td><td>reject-criticism</td><td>@</td></tr>
<tr><td>> Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.<br><br><br><br></td><td>@</td><td>structuring</td><td>@</td></tr>
<tr><td>Besides the important technical difference described above, we highlight the empirical performance of Dreamer.</td><td>@</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>A conclusion of the SVG paper was that the model did not yield substantial practical benefits beyond 1-step predictions.</td><td>@</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>We found it important to revisit this topic in the light of recent substantial improvements to dynamics models (see below).<br><br><br><br></td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>> Effectiveness on very long horizon trajectories: Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.</td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>This is an open issue in model-based RL.<br><br><br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>While current dynamics models still cannot accurately predict full episodes, this is rarely needed in practice.</td><td>*</td><td>contradict-assertion</td><td>*</td></tr>
<tr><td>Recent works successfully use learned dynamics for control from both proprioceptive inputs (Chua et al. 2018, Shyam et al. 2019, Wang & Ba 2019) and from images (Hafner et al. 2019, Zhang et al. 2019).<br><br><br><br></td><td>*</td><td>contradict-assertion</td><td>*</td></tr>
<tr><td>Dreamer shows that the relatively short model predictions (H=20) yield high-quality policy gradients, and that an additional value function in the latent space is effective for solving tasks that require longer-term credit assignment (e.g. with sparse rewards).</td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>Our experiments provide evidence that combination is effective in practice.<br><br><br><br></td><td>*</td><td>summary</td><td>@</td></tr>
<tr><td>> However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).</td><td>@</td><td>structuring</td><td>@</td></tr>
<tr><td>This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).</td><td>@</td><td>structuring</td><td>@</td></tr>
<tr><td>I think this point should be discussed in the paper.</td><td>@</td><td>structuring</td><td>@</td></tr>
<tr><td>That is, the issue still exists, and Dreamer is less effective with very long horizon.<br><br><br><br></td><td>@</td><td>structuring</td><td>@</td></tr>
<tr><td>We address the challenge of long horizons not using long-term model predictions but by learning a value function that estimates the infinite sum of discounted future rewards.</td><td>@</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Figure 4 in our submission shows that this gives Dreamer robustness to the imagination horizon compared to two baselines.<br><br><br><br></td><td>@</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>> Inapplicability to discrete controls:  One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.</td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.</td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.</td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>This restriction should be noted in the paper.<br><br><br><br></td><td>*</td><td>structuring</td><td>*</td></tr>
<tr><td>We applied Dreamer to environments with discrete actions using the DiCE estimator (Foerster et al. 2018) locally for the da/dμ and da/dσ derivatives.</td><td>*</td><td>done</td><td>@</td></tr>
<tr><td>This was a drop-in replacement for the reparameterization estimator and slightly outperformed a Gumble-softmax actor.</td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>We find that with this 1 line change, Dreamer solves discrete action tasks of the Atari suite and a 3D DMLab environment.<br><br><br><br></td><td>*</td><td>answer</td><td>@</td></tr>
<tr><td>> There is no mention about variance of policy gradient estimates.</td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.<br><br><br><br></td><td>@</td><td>structuring</td><td>*</td></tr>
<tr><td>Dreamer uses reparamterization gradients that already have low variance (Kingma & Welling 2013, Rezende et al. 2014); although see Miller et al. (2017).</td><td>@</td><td>reject-criticism</td><td>*</td></tr>
<tr><td>Learning baselines for variance reduction is common for Reinforce estimators as used in A3C and PPO (Mnih et al. 2016, Schulman et al. 2017) but not for reparameterization estimators as used in Dreamer, SVG, and SAC (Heess et al. 2015, Haarnoja et al. 2018).</td><td>@</td><td>reject-criticism</td><td>*</td></tr></table>
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>

