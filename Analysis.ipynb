{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "import json\n",
    "import openreview\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "dataset_dir = \"data_prep/final_dataset/\"\n",
    "\n",
    "SUBSETS = \"train dev test\".split()\n",
    "\n",
    "datasets = collections.defaultdict(list)\n",
    "\n",
    "for subset in SUBSETS:\n",
    "    for filename in glob.glob(dataset_dir + subset + \"/*\"):\n",
    "        with open(filename, 'r') as f:\n",
    "            datasets[subset].append(json.load(f))\n",
    "            \n",
    "all_pairs = sum(datasets.values(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcf67b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def total_and_average_len(list_of_lists):\n",
    "    big_list = sum(list_of_lists, [])\n",
    "    return len(big_list), len(big_list)/len(list_of_lists)\n",
    "\n",
    "def count_dataset(pairs, subset):\n",
    "    # TODO: Add double-annotated and adjudicated\n",
    "    review_total, review_average = total_and_average_len([pair[\"review_sentences\"] for pair in pairs])\n",
    "    rebuttal_total, rebuttal_average = total_and_average_len([pair[\"rebuttal_sentences\"] for pair in pairs])\n",
    "    return {\n",
    "        \"subset\":subset,\n",
    "        \"pairs\": len(pairs),\n",
    "        \"forums\": len(set(pair[\"metadata\"][\"forum_id\"] for pair in pairs)),\n",
    "        \"adjudicated\": len([pair for pair in pairs if pair[\"metadata\"][\"annotator\"] == \"anno0\"]),\n",
    "        \"review_sentences\": review_total,\n",
    "        \"rebuttal_sentences\": rebuttal_total,\n",
    "        \"review_avg_sentences\": review_average,\n",
    "        \"rebuttal_avg_sentences\": rebuttal_average,\n",
    "        \n",
    "    }\n",
    "# Distribution of examples over sets\n",
    "df_dicts = [count_dataset(pairs, subset) for subset, pairs in datasets.items()]\n",
    "dataframe = pd.DataFrame.from_dict(df_dicts)\n",
    "\n",
    "dataframe.round(2).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16927657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearmen\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "spearmen = []\n",
    "for pair in all_pairs:\n",
    "    alignment_list = []\n",
    "    for sentence in pair[\"rebuttal_sentences\"]:\n",
    "        align_type, align_indices = sentence[\"alignment\"]\n",
    "        if align_type == \"context_sentences\":\n",
    "            for review_index in align_indices:\n",
    "                alignment_list.append([sentence[\"sentence_index\"], review_index])\n",
    "    if not alignment_list:\n",
    "        continue\n",
    "    a, b = zip(*alignment_list)\n",
    "    if len(set(a)) == 1 or len(set(b)) == 1:\n",
    "        continue\n",
    "    spearmen.append(stats.spearmanr(*zip(*alignment_list)).correlation)\n",
    "    \n",
    "sns.histplot(spearmen, bins=100, palette=\"crest_r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity v/s aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af24f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type responded to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa663e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of context\n",
    "\n",
    "CONTIGUOUS, NONCONTIGUOUS = \"contiguous noncontiguous\".split()\n",
    "\n",
    "context_counter = collections.Counter()\n",
    "length_counter = {\n",
    "    CONTIGUOUS: collections.Counter(),\n",
    "    NONCONTIGUOUS: collections.Counter()\n",
    "}\n",
    "normalized_length_counter = {\n",
    "    CONTIGUOUS: collections.Counter(),\n",
    "    NONCONTIGUOUS: collections.Counter()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def is_contiguous(alignments):\n",
    "    relevant_range = list(range(min(alignments), max(alignments) + 1))\n",
    "    return relevant_range == list(sorted(alignments))\n",
    "\n",
    "context_type_counter = collections.Counter()\n",
    "\n",
    "for pair in all_pairs:\n",
    "    for sentence in pair[\"rebuttal_sentences\"]:\n",
    "        align_type, align_indices = sentence[\"alignment\"]\n",
    "        if align_type == \"context_sentences\":\n",
    "            assert align_indices\n",
    "            if is_contiguous(align_indices):\n",
    "                context_subtype = CONTIGUOUS\n",
    "                if len(align_indices) == 1:\n",
    "                    align_type = \"single\"\n",
    "                else:\n",
    "                    align_type = CONTIGUOUS\n",
    "            else:\n",
    "                context_subtype = NONCONTIGUOUS\n",
    "                align_type = NONCONTIGUOUS\n",
    "                \n",
    "            context_type_counter[context_subtype] += 1\n",
    "            length_counter[context_subtype][len(align_indices)] += 1\n",
    "            normalized_length_counter[context_subtype][\n",
    "                len(align_indices)/len(pair[\"review_sentences\"])] += 1\n",
    "        \n",
    "        context_type_counter[align_type] += 1\n",
    "        \n",
    "        \n",
    "CONTEXT_TYPE_LABEL_MAP = {\n",
    "    \"context_global\": \"Global context\",\n",
    "\"contiguous\": \"Multiple contiguous\",\n",
    "\"single\": \"Single sentence\",\n",
    "\"context_in-rebuttal\": \"Context in rebuttal\",\n",
    "\"noncontiguous\": \"Multiple non-contiguous\",\n",
    "\"context_none\": \"No context\",\n",
    "\"context_error\": \"Context error\",\n",
    "\"context_unknown\": \"Cannot be determined\",\n",
    "}\n",
    "            \n",
    "context_type_df_dicts = []\n",
    "for k, v in context_type_counter.most_common():\n",
    "    context_type_df_dicts.append({\n",
    "        \"Context type\": CONTEXT_TYPE_LABEL_MAP[k],\n",
    "        \"Num. sents\": v,\n",
    "        \"% sents\": \"{:.2f}%\".format(v * 100/sum(context_type_counter.values()))\n",
    "    })\n",
    "    \n",
    "print(pd.DataFrame.from_dict(context_type_df_dicts).to_latex(index=False))\n",
    "\n",
    "normalized_dataframe_dicts = []\n",
    "for context_subtype, maps in normalized_length_counter.items():\n",
    "    for mapped_to, count in maps.items():\n",
    "        normalized_dataframe_dicts.append({\n",
    "            \"mapped_to\":mapped_to,\n",
    "            \"count\":count,\n",
    "            \"cat\":context_subtype\n",
    "        })\n",
    "normalized_df = pd.DataFrame.from_dict(normalized_dataframe_dicts)\n",
    "\n",
    "length_dataframe_dicts = []\n",
    "for context_subtype, maps in length_counter.items():\n",
    "    for mapped_to, count in maps.items():\n",
    "        length_dataframe_dicts += [{\n",
    "            \"mapped_to\":mapped_to,\n",
    "            \"cat\":context_subtype\n",
    "        }] * count\n",
    "length_df = pd.DataFrame.from_dict(length_dataframe_dicts)\n",
    "            \n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,5))\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "sns.histplot(\n",
    "    data=length_df, x=\"mapped_to\", hue=\"cat\", ax=axes[0], multiple=\"dodge\", log_scale=[False,True], bins=(length_df[\"mapped_to\"].max()+1), palette=\"crest\")\n",
    "\n",
    "sns.kdeplot(data=normalized_df, x=\"mapped_to\", hue=\"cat\", multiple=\"stack\", ax=axes[1], palette=\"crest\")\n",
    "\n",
    "plt.savefig(\"mappedfrac.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(length_dataframe_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreeability v/s variance\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_agreeability(pair_obj):\n",
    "  coarse_counter = collections.Counter()\n",
    "  for sentence in pair_obj[\"rebuttal_sentences\"]:\n",
    "    coarse_counter[sentence['coarse']] += 1\n",
    "\n",
    "  if 'concur' not in coarse_counter and 'dispute' not in coarse_counter:\n",
    "    return {'agreeability': None}\n",
    "\n",
    "  return {\n",
    "      \"agreeability\":\n",
    "          coarse_counter['concur'] /\n",
    "          (coarse_counter['concur'] + coarse_counter['dispute'])\n",
    "  }\n",
    "\n",
    "forum_info_map = collections.defaultdict(list)\n",
    "\n",
    "for pair in all_pairs:\n",
    "    forum_info_map[pair[\"metadata\"][\"forum_id\"]].append((get_agreeability(pair)[\"agreeability\"],pair[\"metadata\"][\"rating\"] ))\n",
    "\n",
    "agreeability_df_dicts = []\n",
    "\n",
    "agree_var, rating_var, avg_rating = [\"Variance in agreeability\", \"Variance in rating\", \"Average rating\"]\n",
    "    \n",
    "for forum, info in forum_info_map.items():\n",
    "    if len(info) == 1:\n",
    "        continue\n",
    "    agreeabilities, ratings = list(zip(*info))\n",
    "    if None in agreeabilities:\n",
    "        continue\n",
    "    agreeability_df_dicts.append({agree_var: np.var(agreeabilities),\n",
    "                                 rating_var: np.var(ratings),\n",
    "                                 avg_rating: np.mean(ratings)})\n",
    "    \n",
    "agreeability_df = pd.DataFrame.from_dict(agreeability_df_dicts)\n",
    "\n",
    "high_agreeability_variance_threshold = agreeability_df.quantile(0.9)[agree_var]\n",
    "low_score_variance_threshold = agreeability_df.quantile(0.9)[rating_var]\n",
    "\n",
    "print(agreeability_df.quantile(0.75)[rating_var])\n",
    "\n",
    "ax = sns.scatterplot(data=agreeability_df, x=agree_var, y=rating_var, hue=avg_rating, palette=\"crest\")\n",
    "plt.plot([high_agreeability_variance_threshold,high_agreeability_variance_threshold], [0,8.5], linestyle=\"dashed\", color=\"green\")\n",
    "plt.plot([0,0.25], [low_score_variance_threshold,low_score_variance_threshold], linestyle=\"dashed\", color=\"green\")\n",
    "\n",
    "plt.savefig(\"agreeability_vs_rating.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "j = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "for example in all_pairs:\n",
    "    review_coarse_labels = [sentence[\"coarse\"] for sentence in example[\"review_sentences\"]]\n",
    "    for rebuttal_sentence in example[\"rebuttal_sentences\"]:\n",
    "        fine = rebuttal_sentence[\"fine\"]\n",
    "        align_type, aligned_idxs = rebuttal_sentence[\"alignment\"]\n",
    "        if align_type == \"context_sentences\":\n",
    "            for aligned_idx in rebuttal_sentence[\"alignment\"][1]:\n",
    "                try:\n",
    "                    j[fine][review_coarse_labels[aligned_idx]] += 1\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "review_types = \"arg_request arg_evaluative arg_fact arg_structuring arg_other\".split()\n",
    "\n",
    "full_eval_responses = [\n",
    "    \"rebuttal_accept-praise\",\n",
    "\"rebuttal_concede-criticism\",\n",
    "\"rebuttal_mitigate-criticism\",\n",
    "\"rebuttal_reject-criticism\",\n",
    "]\n",
    "\n",
    "full_request_responses = [\n",
    "    \"rebuttal_answer\",\n",
    "\"rebuttal_by-cr\",\n",
    "\"rebuttal_done\",\n",
    "\"rebuttal_future\",\n",
    "\"rebuttal_refute-question\",\n",
    "\"rebuttal_reject-request\",\n",
    "]\n",
    "\n",
    "eval_responses = [\n",
    "\"rebuttal_concede-criticism\",\n",
    "]\n",
    "\n",
    "request_responses = [\n",
    "    \"rebuttal_answer\",\n",
    "\"rebuttal_done\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "k=sns.color_palette(\"crest\", 2)\n",
    "print(k)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(eval_responses + request_responses), ncols=1, figsize=(10,8))\n",
    "\n",
    "print(\"\\n\".join(j.keys()))\n",
    "\n",
    "ax_count = 0\n",
    "for key in sorted(eval_responses) + sorted(request_responses):\n",
    "    vals = j[key]\n",
    "    if key in eval_responses:\n",
    "        color = k[0]\n",
    "    elif key in request_responses:\n",
    "        color = k[1]\n",
    "    else:\n",
    "        continue\n",
    "    sns.barplot(x=review_types, y=[vals[i] for i in review_types],ax=axes[ax_count],color=color)\n",
    "    axes[ax_count].set_ylabel(key[9:])\n",
    "    ax_count += 1\n",
    "    \n",
    "plt.savefig(\"transitions.pdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5f180",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review and rebuttal length\n",
    "\n",
    "big_list = sum(datasets.values(), [])\n",
    "\n",
    "length_counter = collections.Counter()\n",
    "\n",
    "for example in big_list:\n",
    "    length_counter[(len(example[\"review_sentences\"]), \n",
    "                    len(example[\"rebuttal_sentences\"]))] += 1\n",
    "df = pd.DataFrame.from_dict([\n",
    "    {\n",
    "        \"review_length\": a,\n",
    "        \"rebuttal_length\": b,\n",
    "        \"count\": count\n",
    "    } for (a, b), count in length_counter.items()\n",
    "])\n",
    "sns.jointplot(x=df.review_length, y=df.rebuttal_length, cmap=\"Blues\", kind='hist')\n",
    "plt.show()\n",
    "plt.savefig('reb-rev-len.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f872c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "guest_client = openreview.Client(baseurl='https://api.openreview.net')\n",
    "\n",
    "def get_num_reviews(guest_client, forum_id):\n",
    "    forum_notes = guest_client.get_notes(forum=forum_id)\n",
    "    review_count = 0\n",
    "    for note in forum_notes:\n",
    "        if note.replyto == forum_id and 'Reviewer' in note.signatures[0]:\n",
    "            review_count += 1\n",
    "    return review_count\n",
    "\n",
    "# for subset, examples in datasets.items():\n",
    "#     forum_counter = collections.defaultdict(list)\n",
    "#     for example in tqdm.tqdm(examples[:100]):\n",
    "#         forum_counter[example[\"metadata\"][\"forum_id\"]].append(example[\"metadata\"][\"review_id\"])\n",
    "    \n",
    "    \n",
    "#     percentage_annotated_list = []\n",
    "\n",
    "#     for forum, annotated_reviews in tqdm.tqdm(forum_counter.items()):\n",
    "#         percentage_annotated_list.append(len(annotated_reviews)/get_num_reviews(guest_client, forum))\n",
    "        \n",
    "#     sns.histplot(data=percentage_annotated_list)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49219b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4ad3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
