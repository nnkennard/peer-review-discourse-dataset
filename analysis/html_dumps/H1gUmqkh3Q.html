
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper.</b> 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] <b>1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA).</b> 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. <b>1. Title of the paper</b> - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] <b>1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA).</b> 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper <b>- We agree that the main highest-level task that we show is VQA, even though our method is more general.</b> Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] <b>1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA).</b> 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. <b>Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA.</b> Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] <b>1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA).</b> 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. <b>Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks.</b> 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). <b>2. Annotation is not clear in this paper.</b> <b>For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).</b> <b>" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.</b> <b>On page 4, State update function, what is the meaning of variable "Epsilon" in the equation?</b> <b>From the supplementary, it seems Epsilon means the environment?</b> 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. <b>2. Description of variables</b> - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). <b>2. Annotation is not clear in this paper.</b> <b>For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).</b> <b>" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.</b> <b>On page 4, State update function, what is the meaning of variable "Epsilon" in the equation?</b> <b>From the supplementary, it seems Epsilon means the environment?</b> 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables <b>- Thanks for the feedback.</b> Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). <b>2. Annotation is not clear in this paper.</b> <b>For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).</b> <b>" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.</b> <b>On page 4, State update function, what is the meaning of variable "Epsilon" in the equation?</b> <b>From the supplementary, it seems Epsilon means the environment?</b> 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. <b>Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables.</b> We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). <b>2. Annotation is not clear in this paper.</b> <b>For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).</b> <b>" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.</b> <b>On page 4, State update function, what is the meaning of variable "Epsilon" in the equation?</b> <b>From the supplementary, it seems Epsilon means the environment?</b> 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. <b>We edited the text to address variables more gently and to explain the arrow sign.</b> 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? <b>3. On the object counting task, the query transmitter needs to produce a query for a relationship module.</b> <b>The authors mentioned that this is softly calculated by softmax on the importance score.</b> <b>Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?</b> 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. <b>3. Query for the relationship module</b> - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? <b>3. On the object counting task, the query transmitter needs to produce a query for a relationship module.</b> <b>The authors mentioned that this is softly calculated by softmax on the importance score.</b> <b>Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?</b> 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module <b>- The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training.</b> When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? <b>3. On the object counting task, the query transmitter needs to produce a query for a relationship module.</b> <b>The authors mentioned that this is softly calculated by softmax on the importance score.</b> <b>Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?</b> 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. <b>When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores.</b> This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? <b>3. On the object counting task, the query transmitter needs to produce a query for a relationship module.</b> <b>The authors mentioned that this is softly calculated by softmax on the importance score.</b> <b>Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?</b> 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. <b>This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients.</b> 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? <b>4. The cider score of image captioning is 109 compared to the baseline 108.</b> <b>The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data.</b> <b>Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input.</b> <b>My assumption is the visual feature already contains the label information for image captioning.</b> 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. <b>4. CIDEr score of captioning</b> - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? <b>4. The cider score of image captioning is 109 compared to the baseline 108.</b> <b>The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data.</b> <b>Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input.</b> <b>My assumption is the visual feature already contains the label information for image captioning.</b> 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning <b>- That may be true to some extent.</b> However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? <b>4. The cider score of image captioning is 109 compared to the baseline 108.</b> <b>The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data.</b> <b>Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input.</b> <b>My assumption is the visual feature already contains the label information for image captioning.</b> 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. <b>However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size.</b> 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. <b>5 and 6.</b> Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. <b>Comparison with SOTA models for counting and relationship detection</b> - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection <b>- To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering.</b> Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. <b>Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018).</b> Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 20<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). <b>Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling.</b> This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 21<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. <b>This shows that additional modules help.</b> Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 22<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. <b>Kim et al. (2018) which is concurrent to our work shows similar performance.</b> For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 23<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. <b>5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.</b> <b>6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?</b> 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. <b>For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.</b> 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 24<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? <b>7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.</b> <b>Is the number right?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. <b>7. Table 4, accuracies are from Zhang et al. 2018</b> - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 25<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? <b>7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.</b> <b>Is the number right?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 <b>- Yes, the numbers are from their paper.</b> One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 26<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? <b>7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.</b> <b>Is the number right?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. <b>One possible explanation for this could be their use of high regularization for a single model instead of ensembling.</b> Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 27<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? <b>7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.</b> <b>Is the number right?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. <b>Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller.</b> (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 28<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. <b>(Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering</b> (Kim et al. 2018) Bilinear Attention Networks (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 29<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering <b>(Kim et al. 2018) Bilinear Attention Networks</b> (Lu et al. 2016) Visual Relationship Detection with Language Priors 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Visual Reasoning by Progressive Module Networks<br/><b>Rebuttal index</b>: 30<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>[Summary] This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality. [Strength] 1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering. This is different from most existing work. 2: By examing different modules, the proposed method is more interpretable compare to canonical methods. 3: The experiment results are good, especially for the counting problem. [Weakness] 1. The title of the paper is "visual reasoning by progressive module networks." The title may be a little overstated since the major task is focused on visual question answering (VQA). 2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, "the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). " There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable "Epsilon" in the equation? From the supplementary, it seems Epsilon means the environment? 3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? 4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. 5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. 6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? 7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper. 1. Title of the paper - We agree that the main highest-level task that we show is VQA, even though our method is more general. Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA. Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks. 2. Description of variables - Thanks for the feedback. Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables. We edited the text to address variables more gently and to explain the arrow sign. 3. Query for the relationship module - The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training. When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores. This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients. 4. CIDEr score of captioning - That may be true to some extent. However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size. 5 and 6. Comparison with SOTA models for counting and relationship detection - To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering. Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018). Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling. This shows that additional modules help. Kim et al. (2018) which is concurrent to our work shows similar performance. For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult. 7. Table 4, accuracies are from Zhang et al. 2018 - Yes, the numbers are from their paper. One possible explanation for this could be their use of high regularization for a single model instead of ensembling. Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller. (Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering (Kim et al. 2018) Bilinear Attention Networks <b>(Lu et al. 2016) Visual Relationship Detection with Language Priors</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
