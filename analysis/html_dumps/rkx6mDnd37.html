
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments.</b> Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. <b>Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.</b> > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. <b>> ...the additions proposed are small modifications to existing algorithms</b> We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms <b>We concede that the modifications to the existing models is a minor contribution.</b> We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. <b>We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details.</b> We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. <b>We plan to make our code public to aid research in the area.</b> To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. <b>To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294).</b> This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). <b>This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers.</b> > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. <b>> ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017).</b> Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). <b>Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types.</b> Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. <b>Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI.</b> In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. <b>In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT.</b> In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. <b>In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d.</b> During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. <b>During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search).</b> This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). <b>This leads us to conclude that vanilla GAT would not perform well on the RDF tasks.</b> As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. <b>As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results.</b> As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. <b>As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile.</b> We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: <b>- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)</b> - impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...) However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. <b>We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour.</b> > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. <b>> ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN</b>  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN <b></b> We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 20<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  <b>We agree that any improvements compared to RGCN are marginal.</b> In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 21<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. <b>In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT.</b> The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 22<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. <b>The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before.</b> We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 23<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. <b>We also see value in reporting these negative results.</b> It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 24<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. <b>It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].</b> The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 25<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. <b>The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.</b> On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 26<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. <b>On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.</b> > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 27<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. <b>> ...often these small variations in results can be compensated with better baselines training</b>  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 28<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training <b></b> We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 29<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  <b>We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true.</b> To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 30<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. <b>To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN.</b> In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 31<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. <b>In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017).</b> On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 32<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). <b>On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance.</b> This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 33<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations. Unfortunately the paper falls short in two main areas: - novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017) <b>- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)</b> However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > ...the additions proposed are small modifications to existing algorithms We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details. We plan to make our code public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294). This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers. > ...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017). Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesnt support relationship types. Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI. In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT. In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d. During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search). This leads us to conclude that vanilla GAT would not perform well on the RDF tasks. As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results. As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile. We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour. > ...the results achieved in the experiments are very small improvements compared to the baseline of RGCN  We agree that any improvements compared to RGCN are marginal. In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT. The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before. We also see value in reporting these negative results. It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > ...often these small variations in results can be compensated with better baselines training  We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true. To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN. In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017). On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance. <b>This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above.</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
