
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>Thanks for your feedback.</b> We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. <b>We discuss each comment in the following:</b> - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: <b>- The experiments are not large scale</b> We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale <b>We respectfully disagree with the reviewer's main comment that the experiments are not large scale.</b> One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. <b>One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).</b> Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). <b>Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).</b> Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). <b>Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.</b> Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. <b>Representation learning, the topic of this conference, has many facets.</b> Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. <b>Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.</b> Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. <b>Both are valuable in different circumstances.</b> - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. <b>- No substantiate insight with respect to NP-hard problems</b> We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems <b>We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.</b> We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. <b>We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.</b> To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. <b>To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives.</b> These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. <b>These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard.</b> Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. <b>Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees.</b> Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. <b>Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML.</b> Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. <b>Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem.</b> So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. <b>So powerful non-convex solvers might be of a significant advantage over convex relaxations.</b> Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. <b>The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.</b> As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. <b>Our paper simply shows ONE example for this.</b> - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 20<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. <b>On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?</b> The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. <b>- It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly?</b> It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 21<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. <b>On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?</b> The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? <b>It would not be possible to set the input dimension the same as the embedding dimension.</b> Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 22<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. <b>On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?</b> The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. <b>Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error.</b> The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 23<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. <b>On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?</b> The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. <b>The size of the embedding dimension can be too low to achieve this.</b> One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 24<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. <b>On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?</b> The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. <b>One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation.</b> However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 25<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. <b>On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?</b> The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. <b>However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding.</b> - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 26<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. <b>- Methods, where items have no representation, are questionable</b> Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 27<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable <b>Items having no representation is a caveat of the data available rather than that of the method.</b> The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 28<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. <b>The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.</b> - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 29<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. <b>- How to generalize to unseen items</b> First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 30<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items <b>First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.</b> We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 31<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. <b>We believe that in our case, generalization is realizable.</b> One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 32<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. <b>One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.</b> The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 33<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? <b>The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.</b> The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. <b>The network can be trained with extra batches of triplets which involves the new items.</b> - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 34<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. <b>The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. <b>- The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.</b> We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: LARGE SCALE REPRESENTATION LEARNING FROM TRIPLET COMPARISONS<br/><b>Rebuttal index</b>: 35<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to "directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. As such the paper is not convincing. On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly? The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable. <b>The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Thanks for your feedback. We discuss each comment in the following: - The experiments are not large scale We respectfully disagree with the reviewer's main comment that the experiments are not large scale. One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4). Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches). Sure, this is not the scale of 80 million tiny images; but one wouldnt ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances. Representation learning, the topic of this conference, has many facets. Learning representations from big data (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side. Both are valuable in different circumstances. - No substantiate insight with respect to NP-hard problems We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem. We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems. To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives. These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard. Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees. Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML. Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem. So powerful non-convex solvers might be of a significant advantage over convex relaxations. Our paper simply shows ONE example for this. - It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly? It would not be possible to set the input dimension the same as the embedding dimension. Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error. The size of the embedding dimension can be too low to achieve this. One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation. However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding. - Methods, where items have no representation, are questionable Items having no representation is a caveat of the data available rather than that of the method. The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework. - How to generalize to unseen items First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage. We believe that in our case, generalization is realizable. One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items. The network can be trained with extra batches of triplets which involves the new items. - The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning. <b>We dont really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this.</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
