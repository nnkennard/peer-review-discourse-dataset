
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments.</b> Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. <b>Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.</b> > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. <b>i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).</b> Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. <b>> i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).</b> Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). <b>Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.</b> ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). <b>Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.</b> We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. <b>i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).</b> <b>Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.</b> ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>We concede that the modifications to the existing models is a minor contribution.</b> We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. <b>i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).</b> <b>Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.</b> ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. <b>We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.</b> We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. <b>i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).</b> <b>Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.</b> ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. <b>We plan to make our implementation public to aid research in the area.</b> To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_none<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. <b>To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs.</b> We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_none<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. <b>We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution.</b> > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. <b>> ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse.</b> We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. <b>We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way.</b> This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. <b>This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT.</b> These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. <b>These results will be included in the new manuscript.</b> We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. <b>We also feel that some of the results being significantly worse is one of the main contributions of our paper.</b> Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. <b>Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart.</b> This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. <b>This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].</b> The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. <b>The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.</b> On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. <b>ii) In table 2, I dont really see any promising results compared to baselines. There are</b> <b>little improvements over the baselines or even significantly worse. More importantly,</b> compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. <b>On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.</b> > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. <b>> iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? <b>The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description.</b> There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 20<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. <b>There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2].</b> In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 21<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. <b>In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form</b> d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 22<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form <b>d1 -> hasAtom -> d1_1</b> d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 23<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 <b>d1 -> hasBond -> bond1</b> d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 24<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 <b>d1- > hasStructure -> ring_size_6-1</b> where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 25<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 <b>where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on.</b> There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 26<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. <b>There are many more types than this, and are viewable in the .owl located at [2].</b> Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 27<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. <b>Nodes correspond to entities from the point of view of RDF.</b> Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 28<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. <b>Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation).</b> This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 29<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). <b>This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled).</b> Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 30<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). <b>Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules.</b> If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 31<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella <b>Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. <b>If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective.</b> [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 32<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. <b>[1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis</b> [2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Relational Graph Attention Networks<br/><b>Rebuttal index</b>: 33<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications. The basic idea is to combine the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks. This paper is generally easy to follow and written clearly. Several experiments are conducted to demonstrate the performance of the proposed model. Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model. i) The proposed architecture is mainly adopted from the graph attention networks (Velickovic et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. More importantly, compared two schemes of this work, the ones with attentions are almost identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more). iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use. MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels. Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem? 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments. Hopefully the new results in our response will better aid discussion. Your specific points are addressed below. > i) The proposed architecture is mainly adopted from the graph attention networks (Velikovi  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018). Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited. We concede that the modifications to the existing models is a minor contribution. We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial. We plan to make our implementation public to aid research in the area. To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs. We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the papers contribution. > ii) In table 2, I dont really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse. We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way. This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT. These results will be included in the new manuscript. We also feel that some of the results being significantly worse is one of the main contributions of our paper. Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart. This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)]. The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem. On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue. > iii) Could you explain why your MUTAG is now a single graph and is cast as node classification problem? The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description. There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2]. In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form d1 -> hasAtom -> d1_1 d1 -> hasBond -> bond1 d1- > hasStructure -> ring_size_6-1 where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on. There are many more types than this, and are viewable in the .owl located at [2]. Nodes correspond to entities from the point of view of RDF. Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation). This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled). Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules. If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective. [1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis <b>[2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
