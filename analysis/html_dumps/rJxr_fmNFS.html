
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>We would like to thank reviewer #1 for the constructive critics.</b> We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. <b>We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup.</b> We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. <b>We have updated the paper pdf and kindly ask the reviewer to take another look.</b> THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. <b>THEORY:</b> Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: <b>Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0.</b> Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. <b>Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work.</b> Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. <b>Theorem 1 formally states when k-addition is defined for spherical spaces.</b> Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. <b>Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem.</b> Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. <b>Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match.</b> It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. <b>It also gives the derivative of the distance function w.r.t. curvature around 0.</b> Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. <b>Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative.</b> Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. <b>Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.</b> This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. <b>This is a desirable property for Riemannian averaging.</b> Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. <b>This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.</b> There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. <b>Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.</b> LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. <b>(3) A large body of graph neural network literature is omitted.</b> <b>The authors start from a very high-level description of machine learning in constant curvature spaces.</b> <b>Such high-level introductions require more comprehensive literatures to support.</b> <b>In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN.</b> <b>This is misleading.</b> <b>For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.</b> <b>The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.</b> <b>This is a thriving area that requires a careful literature review.</b> (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. <b>LITERATURE:</b> A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. <b>(3) A large body of graph neural network literature is omitted.</b> <b>The authors start from a very high-level description of machine learning in constant curvature spaces.</b> <b>Such high-level introductions require more comprehensive literatures to support.</b> <b>In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN.</b> <b>This is misleading.</b> <b>For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.</b> <b>The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.</b> <b>This is a thriving area that requires a careful literature review.</b> (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: <b>A more careful review of the literature has been incorporated into the introduction section.</b> We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. <b>(3) A large body of graph neural network literature is omitted.</b> <b>The authors start from a very high-level description of machine learning in constant curvature spaces.</b> <b>Such high-level introductions require more comprehensive literatures to support.</b> <b>In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN.</b> <b>This is misleading.</b> <b>For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.</b> <b>The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.</b> <b>This is a thriving area that requires a careful literature review.</b> (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. <b>We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline).</b> WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. <b>(4) The writing quality is not satisfactory.</b> <b>Here are a few examples: The ICLR citation style needs to use sometimes \citep.</b> <b>The authors instead used \cite everywhere, making the paper hard to read.</b> <b>The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).</b> <b>The introduction can start at a lower level (such as flat/hyperbolic neural networks).</b> <b>Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). <b>WRITING:</b> We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. <b>(4) The writing quality is not satisfactory.</b> <b>Here are a few examples: The ICLR citation style needs to use sometimes \citep.</b> <b>The authors instead used \cite everywhere, making the paper hard to read.</b> <b>The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).</b> <b>The introduction can start at a lower level (such as flat/hyperbolic neural networks).</b> <b>Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: <b>We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.</b> We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. <b>(4) The writing quality is not satisfactory.</b> <b>Here are a few examples: The ICLR citation style needs to use sometimes \citep.</b> <b>The authors instead used \cite everywhere, making the paper hard to read.</b> <b>The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).</b> <b>The introduction can start at a lower level (such as flat/hyperbolic neural networks).</b> <b>Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.</b> 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. <b>We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.</b> MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 20<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. <b>(2) The method is not well motivated.</b> <b>The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.</b> <b>This is too general and not enough as a motivation.</b> <b>After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.</b> (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. <b>MOTIVATION:</b> We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 21<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. <b>(2) The method is not well motivated.</b> <b>The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.</b> <b>This is too general and not enough as a motivation.</b> <b>After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.</b> (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: <b>We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data.</b> This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 22<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. <b>(2) The method is not well motivated.</b> <b>The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.</b> <b>This is too general and not enough as a motivation.</b> <b>After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.</b> (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. <b>This is in accordance with the previous related work on non-Euclidean embeddings.</b> Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 23<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. <b>(2) The method is not well motivated.</b> <b>The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.</b> <b>This is too general and not enough as a motivation.</b> <b>After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.</b> (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. <b>Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.</b> EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 24<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. <b>(1) The experimental results cannot show the usefulness of the proposed GCN.</b> <b>The results on real datasets are similar to the regular GCN.</b> <b>As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN".</b> <b>The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.</b> (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. <b>EXPERIMENTAL SETTINGS & HYPERPARAMETERS:</b> Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 25<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. <b>(1) The experimental results cannot show the usefulness of the proposed GCN.</b> <b>The results on real datasets are similar to the regular GCN.</b> <b>As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN".</b> <b>The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.</b> (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: <b>Are added to section 4 and appendix E</b> EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 26<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. <b>(1) The experimental results cannot show the usefulness of the proposed GCN.</b> <b>The results on real datasets are similar to the regular GCN.</b> <b>As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN".</b> <b>The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.</b> (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E <b>EXPERIMENTAL RESULTS:</b> Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 27<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. <b>(1) The experimental results cannot show the usefulness of the proposed GCN.</b> <b>The results on real datasets are similar to the regular GCN.</b> <b>As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN".</b> <b>The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.</b> (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: <b>Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification.</b> We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 28<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. <b>(1) The experimental results cannot show the usefulness of the proposed GCN.</b> <b>The results on real datasets are similar to the regular GCN.</b> <b>As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN".</b> <b>The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.</b> (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. <b>We believe further experimental investigations are needed to better train non-Euclidean graph neural network models.</b> [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 29<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. <b>[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19</b> [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 30<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 <b>[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19</b> We would love to hear your feedback on our substantially improved paper (based on your suggestions). Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 31<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 <b>We would love to hear your feedback on our substantially improved paper (based on your suggestions).</b> Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you! 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Constant Curvature Graph Convolutional Networks<br/><b>Rebuttal index</b>: 32<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_global<br /> <br/>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement. This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented. I vote for rejection for four major weaknesses explained as follows. (1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given. (2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same. (3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review. (4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \citep. The authors instead used \cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 We would like to thank reviewer #1 for the constructive critics. We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup. We have updated the paper pdf and kindly ask the reviewer to take another look. THEORY: Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0. Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work. Theorem 1 formally states when k-addition is defined for spherical spaces. Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem. Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match. It also gives the derivative of the distance function w.r.t. curvature around 0. Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative. Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication. This is a desirable property for Riemannian averaging. Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries. LITERATURE: A more careful review of the literature has been incorporated into the introduction section. We have also incorporated the recent works [1,2] to appear soon at Neurips19 (their text became available very recently and after the ICLR submission deadline). WRITING: We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings. We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation. MOTIVATION: We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data. This is in accordance with the previous related work on non-Euclidean embeddings. Moreover, we have detailed some limitations of Euclidean spaces in Appendix B. EXPERIMENTAL SETTINGS & HYPERPARAMETERS: Are added to section 4 and appendix E EXPERIMENTAL RESULTS: Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification. We believe further experimental investigations are needed to better train non-Euclidean graph neural network models. [1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips19 [2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips19 We would love to hear your feedback on our substantially improved paper (based on your suggestions). <b>Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you!</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
