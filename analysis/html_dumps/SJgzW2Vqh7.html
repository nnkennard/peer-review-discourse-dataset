
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>1. Comment:</b> Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: <b>1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.</b> The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: <b>Missing comparison with parameter counting bounds [1, 2].</b> 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. <b>1. Response:</b> [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: <b>1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.</b> The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: <b>[1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results.</b> Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: <b>1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.</b> The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. <b>Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions.</b> Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: <b>1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.</b> The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. <b>Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation).</b> [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: <b>1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.</b> The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). <b>[2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear.</b> 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. <b>2. Comment:</b> Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: <b>Vacuous bounds in the regime \beta >1.</b> 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. <b>2. Response:</b> We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: <b>We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous.</b> Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. <b>Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices.</b> The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. <b>The exponential term stems from the layer wise covering argument rather than the range of the output.</b> The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. <b>The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings.</b> Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. <b>Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs.</b> This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. <b>This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5].</b> We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. <b>2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1.</b> <b>A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.</b> <b>They can indeed be subsumed by generalization bounds based on VC theory.</b> <b>The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.</b> 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. <b>We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs.</b> 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. <b>3. Comment:</b> Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: <b>Technical contribution: marginal.</b> 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. <b>3. Response:</b> We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 20<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: <b>We provide new understandings of RNNs by connecting their generalization properties to their empirical success.</b> We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 21<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. <b>We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs.</b> In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 22<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. <b>In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2).</b> This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 23<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). <b>This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t.</b> We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 24<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. <b>We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs.</b> The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 25<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. <b>The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument.</b> Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 26<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. <b>Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1.</b> To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 27<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. <b>To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3):</b> 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 28<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): <b>1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds.</b> 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 29<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. <b>2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5).</b> Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 30<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. <b>3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]</b> 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). <b>Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions.</b> 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 31<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] <b>4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants.</b> <b>It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.</b> [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. <b>4. Comment:</b> Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 32<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] <b>4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants.</b> <b>It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.</b> [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: <b>Missing experiments to validate nature of bounds.</b> 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 33<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] <b>4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants.</b> <b>It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.</b> [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. <b>4. Response:</b> Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 34<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] <b>4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants.</b> <b>It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.</b> [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: <b>Please refer to the revised version for numerical evaluations in Section 6.</b> In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 35<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] <b>4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants.</b> <b>It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.</b> [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. <b>In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1.</b> References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 36<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. <b>References</b> [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 37<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References <b>[1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79.</b> [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 38<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. <b>[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996.</b> [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 39<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. <b>[3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017).</b> [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 40<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). <b>[4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016.</b> [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 41<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. <b>[5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).</b> [6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: On Generalization Bounds of a Family of Recurrent Neural Networks<br/><b>Rebuttal index</b>: 42<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_in-rebuttal<br /> <br/>This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points: 1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison. 2. Vacuous bounds in the regime \beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996. [3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 1. Comment: Missing comparison with parameter counting bounds [1, 2]. 1. Response: [1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results. Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions. Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation). [2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear. 2. Comment: Vacuous bounds in the regime \beta >1. 2. Response: We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous. Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices. The exponential term stems from the layer wise covering argument rather than the range of the output. The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings. Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs. This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5]. We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \beta \approx 1 helps balance the generalization and representation of RNNs. 3. Comment: Technical contribution: marginal. 3. Response: We provide new understandings of RNNs by connecting their generalization properties to their empirical success. We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs. In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t. We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs. The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta > 1. To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3): 1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. 2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5). Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions. 4. Comment: Missing experiments to validate nature of bounds. 4. Response: Please refer to the revised version for numerical evaluations in Section 6. In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \beta > 1. References [1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86, no. 1 (1998): 63-79. [2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." In Advances in Neural Information Processing Systems, pp. 204-210. 1996. [3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On orthogonality and learning recurrent networks with long term dependencies." arXiv preprint arXiv:1702.00071 (2017). [4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. "Unitary evolution recurrent neural networks." In International Conference on Machine Learning, pp. 1120-1128. 2016. [5] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). <b>[6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017.</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
