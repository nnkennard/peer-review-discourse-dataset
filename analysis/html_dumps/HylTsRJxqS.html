
<HTML>
   <head>
      <title> Review-rebuttal alignment viewer </title>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma-rtl.min.css">
   </head>
   <body>
 <div class="container">

 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 0<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: <b>Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?</b> For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 <b>Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?</b> A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 1<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: <b>Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?</b> For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? <b>A: Yes we did and interestingly, it didn't show improvement.</b> The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 2<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: <b>Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?</b> For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. <b>The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4.</b> Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 3<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: <b>Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?</b> For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. <b>Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together.</b> Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 4<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? <b>For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?</b> Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. <b>Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?</b> A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 5<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? <b>For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?</b> Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? <b>A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1.</b> So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 6<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? <b>For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?</b> Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. <b>So latents are canonicalized in both possible orderings.</b> We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 7<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? <b>For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?</b> Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. <b>We discuss this point at the bottom of page 4 after equation 6 and will further clarify.</b> Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 8<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? <b>Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).</b> There might be other constructions that are more efficient and less restrictive. I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. <b>Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).</b> There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 9<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). <b>There might be other constructions that are more efficient and less restrictive.</b> I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). <b>There might be other constructions that are more efficient and less restrictive.</b> A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 10<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? <b>Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).</b> <b>There might be other constructions that are more efficient and less restrictive.</b> I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work.</b> While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 11<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? <b>Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).</b> <b>There might be other constructions that are more efficient and less restrictive.</b> I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. <b>While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement.</b> First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 12<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? <b>Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).</b> <b>There might be other constructions that are more efficient and less restrictive.</b> I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. <b>First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar.</b> However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 13<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? <b>Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).</b> <b>There might be other constructions that are more efficient and less restrictive.</b> I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. <b>However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further.</b> Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 14<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. <b>Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 15<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? <b>A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated).</b> Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 16<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). <b>Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance.</b> If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 17<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. <b>If the above mentioned assumption indeed holds, the font should be the only change in the set.</b> We order the samples according to that axis and plot them in a row from left to right. We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 18<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. <b>We order the samples according to that axis and plot them in a row from left to right.</b> We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis. 
             </div>
          </div>
       </div>
    </div>
 </div>


 <h1 class="subtitle"> <b>Title</b>: Representation Learning Through Latent Canonicalizations<br/><b>Rebuttal index</b>: 19<br> </h1>
 <div class="columns">
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                <b>Context type</b>: context_sentences<br /> <br/>This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations. These transformations are called canonicalizations in the paper. The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor. The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation. It also proposes a loss function and sampling scheme for training the model. The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation. The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set. I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model. A couple things I thought were missing in the paper: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. <b>I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?</b> Minor comments: * "data  tripets" on page 2 * Figure 5 should appear after Figure 4. 
             </div>
          </div>
       </div>
    </div>
    <div class="column">
       <div class="card">
          <div class="card-content">
             <div class="content">
                 Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time? A: Yes we did and interestingly, it didn't show improvement. The results are reported in Appendix A, Table A1 in the row titled: "Ours + classifier after" and discussed in Section 4.2.4. Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together. Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results? A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1. So latents are canonicalized in both possible orderings. We discuss this point at the bottom of page 4 after equation 6 and will further clarify. Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric). There might be other constructions that are more efficient and less restrictive. A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work. While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement. First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar. However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further. Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them? A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated). Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance. If the above mentioned assumption indeed holds, the font should be the only change in the set. We order the samples according to that axis and plot them in a row from left to right. <b>We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis.</b> 
             </div>
          </div>
       </div>
    </div>
 </div>

</div>
   </body>
</HTML>
